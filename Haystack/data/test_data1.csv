description,readme
Bolts is a collection of low-level libraries designed to make developing mobile apps easier.,"b'Bolts\n============\n\n\n\n\n\n\n\n\n\nBolts is a collection of low-level libraries designed to make developing mobile\napps easier. Bolts was designed by Parse and Facebook for our own internal use,\nand we have decided to open source these libraries to make them available to\nothers. Using these libraries does not require using any Parse services. Nor\ndo they require having a Parse or Facebook developer account.\n\nBolts includes:\n\n* ""Tasks"", which make organization of complex asynchronous code more manageable. A task is kind of like a JavaScript Promise, but available for iOS and Android.\n* An implementation of the , helping you link to content in other apps and handle incoming deep-links.\n\nFor more information, see the .\n\n# Tasks\n\nTo build a truly responsive iOS application, you must keep long-running operations off of the UI thread, and be careful to avoid blocking anything the UI thread might be waiting on. This means you will need to execute various operations in the background. To make this easier, weve added a class called . A task represents the result of an asynchronous operation. Typically, a  is returned from an asynchronous function and gives the ability to continue processing the result of the task. When a task is returned from a function, its already begun doing its job. A task is not tied to a particular threading model: it represents the work being done, not where it is executing. Tasks have many advantages over other methods of asynchronous programming, such as callbacks.  is not a replacement for  or GCD. In fact, they play well together. But tasks do fill in some gaps that those technologies dont address.\n*  takes care of managing dependencies for you. Unlike using  for dependency management, you dont have to declare all dependencies before starting a . For example, imagine you need to save a set of objects and each one may or may not require saving child objects. With an , you would normally have to create operations for each of the child saves ahead of time. But you dont always know before you start the work whether thats going to be necessary. That can make managing dependencies with  very painful. Even in the best case, you have to create your dependencies before the operations that depend on them, which results in code that appears in a different order than it executes. With , you can decide during your operations work whether there will be subtasks and return the other task in just those cases.\n*  release their dependencies.  strongly retains its dependencies, so if you have a queue of ordered operations and sequence them using dependencies, you have a leak, because every operation gets retained forever.  release their callbacks as soon as they are run, so everything cleans up after itself. This can reduce memory use, and simplify memory management.\n*  keep track of the state of finished tasks: It tracks whether there was a returned value, the task was cancelled, or if an error occurred. It also has convenience methods for propagating errors. With , you have to build all of this stuff yourself.\n*  dont depend on any particular threading model. So its easy to have some tasks perform their work with an operation queue, while others perform work using blocks with GCD. These tasks can depend on each other seamlessly.\n* Performing several tasks in a row will not create nested ""pyramid"" code as you would get when using only callbacks.\n*  are fully composable, allowing you to perform branching, parallelism, and complex error handling, without the spaghetti code of having many named callbacks.\n* You can arrange task-based code in the order that it executes, rather than having to split your logic across scattered callback functions.\n\nFor the examples in this doc, assume there are async versions of some common Parse methods, called  and  which return a . In a later section, well show how to define these functions yourself.\n\n## The  Method\n\nEvery  has a method named  which takes a continuation block. A continuation is a block that will be executed when the task is complete. You can then inspect the task to check if it was successful and to get its result.\n\n\n\n\n\nBFTasks use Objective-C blocks, so the syntax should be pretty straightforward. Lets look closer at the types involved with an example.\n\n\n\n\n\nIn many cases, you only want to do more work if the previous task was successful, and propagate any errors or cancellations to be dealt with later. To do this, use the  method instead of .\n\n\n\n\n\n## Chaining Tasks Together\n\nBFTasks are a little bit magical, in that they let you chain them without nesting. If you return a BFTask from , then the task returned by  will not be considered finished until the new task returned from the new continuation block. This lets you perform multiple actions without incurring the pyramid code you would get with callbacks. Likewise, you can return a  from . So, return a  to do more asynchronous work.\n\n\n\n\n\n## Error Handling\n\nBy carefully choosing whether to call  or , you can control how errors are propagated in your application. Using  lets you handle errors by transforming them or dealing with them. You can think of failed tasks kind of like throwing an exception. In fact, if you throw an exception inside a continuation, the resulting task will be faulted with that exception.\n\n\n\n\n\nIts often convenient to have a long chain of success callbacks with only one error handler at the end.\n\n## Creating Tasks\n\nWhen youre getting started, you can just use the tasks returned from methods like  or . However, for more advanced scenarios, you may want to make your own tasks. To do that, you create a . This object will let you create a new , and control whether it gets marked as finished or cancelled. After you create a , youll need to call , , or  to trigger its continuations.\n\n\n\n\n\nIf you know the result of a task at the time it is created, there are some convenience methods you can use.\n\n\n\n\n\n## Creating Async Methods\n\nWith these tools, its easy to make your own asynchronous functions that return tasks. For example, you can make a task-based version of  easily.\n\n\n\n\n\nIts similarly easy to create ,  or .\n\n## Tasks in Series\n\n are convenient when you want to do a series of tasks in a row, each one waiting for the previous to finish. For example, imagine you want to delete all of the comments on your blog.\n\n\n\n\n\n## Tasks in Parallel\n\nYou can also perform several tasks in parallel, using the  method. You can start multiple operations at once, and use  to create a new task that will be marked as completed when all of its input tasks are completed. The new task will be successful only if all of the passed-in tasks succeed. Performing operations in parallel will be faster than doing them serially, but may consume more system resources and bandwidth.\n\n\n\n\n\n## Task Executors\n\nBoth  and  methods have another form that takes an instance of . These are  and . These methods allow you to control how the continuation is executed. The default executor will dispatch to GCD, but you can provide your own executor to schedule work onto a different thread. For example, if you want to continue with work on the UI thread:\n\n\n\nFor common cases, such as dispatching on the main thread, we have provided default implementations of . These include , , , , and . For example:\n\n\n\n## Task Cancellation\n\nIts generally bad design to keep track of the  for cancellation. A better model is to create a ""cancellation token"" at the top level, and pass that to each async function that you want to be part of the same ""cancelable operation"". Then, in your continuation blocks, you can check whether the cancellation token has been cancelled and bail out early by returning a . For example:\n\n\n\nNote: The cancellation token implementation should be thread-safe.\nWe are likely to add some concept like this to Bolts at some point in the future.\n\n# App Links\n\n provide a cross-platform mechanism that allows a developer to define and publish a deep-linking scheme for their content, allowing other apps to link directly to an experience optimized for the device they are running on. Whether you are building an app that receives incoming links or one that may link out to other apps content, Bolts provides tools to simplify implementation of the .\n\n## Handling an App Link\n\nThe most common case will be making your app receive App Links. In-linking will allow your users to quickly access the richest, most native-feeling presentation of linked content on their devices. Bolts makes it easy to handle an inbound App Link (as well as general inbound deep-links) by providing utilities for processing an incoming URL.\n\nFor example, you can use the  utility class to parse an incoming URL in your :\n\n\n\n## Navigating to a URL\n\nFollowing an App Link allows your app to provide the best user experience (as defined by the receiving app) when a user navigates to a link. Bolts makes this process simple, automating the steps required to follow a link:\n\n1. Resolve the App Link by getting the App Link metadata from the HTML at the URL specified.\n2. Step through App Link targets relevant to the device being used, checking whether the app that can handle the target is present on the device.\n3. If an app is present, build a URL with the appropriate al_applink_data specified and navigate to that URL.\n4. Otherwise, open the browser with the original URL specified.\n\nIn the simplest case, it takes just one line of code to navigate to a URL that may have an App Link:\n\n\n\n### Adding App and Navigation Data\n\nUnder most circumstances, the data that will need to be passed along to an app during a navigation will be contained in the URL itself, so that whether or not the app is actually installed on the device, users are taken to the correct content. Occasionally, however, apps will want to pass along data that is relevant for app-to-app navigation, or will want to augment the App Link protocol with information that might be used by the app to adjust how the app should behave (e.g. showing a link back to the referring app).\n\nIf you want to take advantage of these features, you can break apart the navigation process. First, you must have an App Link to which you wish to navigate:\n\n\n\nThen, you can build an App Link request with any additional data you would like and navigate:\n\n\n\n### Resolving App Link Metadata\n\nBolts allows for custom App Link resolution, which may be used as a performance optimization (e.g. caching the metadata) or as a mechanism to allow developers to use a centralized index for obtaining App Link metadata. A custom App Link resolver just needs to be able to take a URL and return a  containing the ordered list of s that are applicable for this device. Bolts provides one of these out of the box that performs this resolution on the device using a hidden ~~UIWebView~~ WKWebview.\n\nYou can use any resolver that implements the  protocol by using one of the overloads on :\n\n\n\nAlternatively, a you can swap out the default resolver to be used by the built-in APIs:\n\n\n\n## App Link Return-to-Referer View\n\nWhen an application is opened via an App Link, a banner allowing the user to ""Touch to return to "" should be displayed. The  provides this functionality. It will take an incoming App Link and parse the referer information to display the appropriate calling app name.\n\n\n\nThe following code assumes that the view controller has an   property that has already been populated with the URL used to open the app. You can then do something like this to show the view:\n\n\n\nIn a navigation-controller view hierarchy, the banner should be displayed above the navigation bar, and  provides an  method to assist with this.\n\n## Analytics\n\nBolts introduces Measurement Event. App Links posts three different Measurement Event notifications to the application, which can be caught and integrated with existing analytics components in your application.\n\n*   \xe2\x80\x94 Raised when your app switches out to an App Links URL.\n*   \xe2\x80\x94 Raised when your app opens an incoming App Links URL.\n*   \xe2\x80\x94 Raised when your app returns back the referrer app using the built-in top navigation back bar view.\n\n### Listen for App Links Measurement Events\n\nThere are other analytics tools that are integrated with Bolts App Links events, but you can also listen for these events yourself:\n\n\n\n### App Links Event Fields\n\nApp Links Measurement Events sends additional information from App Links Intents in flattened string key value pairs. Here are some of the useful fields for the three events.\n\n* \n  * : the URL that opens the app.\n  * : the scheme of .\n  * : the URL that the referrer app added into : .\n  * : the app name that the referrer app added to : .\n  * : the bundle of referrer application.\n  * : the  field in .\n  * : App Links API  version.\n\n*  / \n  * : the URL used to open the other app (or browser). If there is an eligible app to open, this will be the custom scheme url/intent in .\n  * : the scheme of .\n  * : the URL of the page hosting App Links meta tags.\n  * : the hostname of .\n  * :  to indicate success in opening the App Link in another app or browser;  to indicate failure to open the App Link.\n  * :  for open in app,  for open in browser;  when the success field is .\n  * : App Links API version.\n\n# Installation\n\nYou can download the latest framework files from our .\n\nBolts is also available through . To install it simply add the following line to your Podfile:\n\n    pod Bolts\n'"
A curated list of awesome computer vision resources,"b'\n# Awesome Computer Vision: \nA curated list of awesome computer vision resources, inspired by .\n\nFor a list people in computer vision listed with their academic genealogy, please visit \n\n## Contributing\nPlease feel free to send me  or email (jbhuang@vt.edu) to add links.\n\n## Table of Contents\n\n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n\n## Awesome Lists\n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n \n\n## Books\n\n#### Computer Vision\n*  - Simon J. D. Prince 2012\n*  - Rick Szeliski 2010\n*  - David Forsyth and Jean Ponce 2011\n*  - Richard Hartley and Andrew Zisserman 2004\n*  - Linda G. Shapiro 2001\n*  - Stephen E. Palmer 1999\n*  - Kristen Grauman and Bastian Leibe 2011\n*  - Richard J. Radke, 2012\n*  - Reinhard, E., Heidrich, W., Debevec, P., Pattanaik, S., Ward, G., Myszkowski, K 2010\n*  - Justin Solomon 2015\n*  - Stan Birchfield 2018\n*  - Silvio Savarese 2018\n\n#### OpenCV Programming\n*  - Gary Bradski and Adrian Kaehler\n*  - Adrian Rosebrock\n*  - Oscar Deniz Suarez, M\xc2\xaa del Milagro Fernandez Carrobles, Noelia Vallez Enano, Gloria Bueno Garcia, Ismael Serrano Gracia\n\n#### Machine Learning\n*  - Christopher M. Bishop 2007\n*  - Christopher M. Bishop 1995\n*  - Daphne Koller and Nir Friedman 2009\n*  - Peter E. Hart, David G. Stork, and Richard O. Duda 2000\n*  - Tom M. Mitchell 1997\n*  - Carl Edward Rasmussen and Christopher K. I. Williams 2005\n* - Yaser S. Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin 2012\n*  - Michael Nielsen 2014\n*  - David Barber, Cambridge University Press, 2012\n\n#### Fundamentals\n *  - Gilbert Strang 1995\n\n## Courses\n\n#### Computer Vision\n *  - William Hoff (Colorado School of Mines)\n *  - Alexei A. Efros and Trevor Darrell (UC Berkeley)\n *  - Steve Seitz (University of Washington)\n * Visual Recognition ,  - Kristen Grauman (UT Austin)\n *  - Tamara Berg (UNC Chapel Hill)\n *  - Fei-Fei Li and Andrej Karpathy (Stanford University)\n *  - Rob Fergus (NYU)\n *  - Derek Hoiem (UIUC)\n *  - Kalanit Grill-Spector and Fei-Fei Li (Stanford University)\n *  - Fei-Fei Li (Stanford University)\n *  - Antonio Torralba and Bill Freeman (MIT)\n *  - Bastian Leibe (RWTH Aachen University)\n *  - Bastian Leibe (RWTH Aachen University)\n *  Pascal Fua (EPFL):\n *  Carsten Rother (TU Dresden):\n *  Carsten Rother (TU Dresden):\n *  Daniel Cremers (TU Munich):\n\n\n\n\n#### Computational Photography\n*  - Alexei A. Efros (UC Berkeley)\n*  - Alexei A. Efros (CMU)\n*  - Derek Hoiem (UIUC)\n*  - James Hays (Brown University)\n*  - Fredo Durand (MIT)\n*  - Ramesh Raskar (MIT Media Lab)\n*  - Irfan Essa (Georgia Tech)\n*  - Stanford University\n*  - Rob Fergus (NYU)\n*  - Kyros Kutulakos (University of Toronto)\n*  - Kyros Kutulakos (University of Toronto)\n*  - Rich Radke (Rensselaer Polytechnic Institute)\n*  - Rich Radke (Rensselaer Polytechnic Institute)\n\n#### Machine Learning and Statistical Learning\n *  - Andrew Ng (Stanford University)\n *  - Yaser S. Abu-Mostafa (Caltech)\n *  - Trevor Hastie and Rob Tibshirani (Stanford University)\n *  - Tomaso Poggio, Lorenzo Rosasco, Carlo Ciliberto, Charlie Frogner, Georgios Evangelopoulos, Ben Deen (MIT)\n *  - Genevera Allen (Rice University)\n *  - Michael Jordan (UC Berkeley)\n *  - David MacKay (University of Cambridge)\n *  - Lester Mackey (Stanford)\n *  - Andrew Zisserman (University of Oxford)\n *  - Sebastian Thrun (Stanford University)\n *  - Charles Isbell, Michael Littman (Georgia Tech)\n *  - Fei-Fei Li, Andrej Karphaty, Justin Johnson (Stanford University)\n *  - Rudolph Triebel (TU Munich)\n\n\n\n#### Optimization\n *  - Stephen Boyd (Stanford University)\n *  - Stephen Boyd (Stanford University)\n *  - Stephen Boyd (Stanford University)\n *  - (MIT)\n *  - Ryan Tibshirani (CMU)\n\n## Papers\n\n#### Conference papers on the web\n *  - Computer vision papers on the web\n *  - Graphics papers on the web\n *  - NIPS papers on the web\n * \n *  - Keith Price (USC)\n *  - (USC)\n\n#### Survey Papers\n * \n * \n * \n\n ## Pre-trained Computer Vision Models\n *  These models are trained on custom objects\n\n## Tutorials and talks\n\n#### Computer Vision\n *  - Lectures, keynotes, panel discussions on computer vision\n *  - Jitendra Malik (UC Berkeley) 2013\n *  - Andrew Blake (Microsoft Research) 2008\n *  - Jitendra Malik (UC Berkeley) 2008\n *  - Fatih Porikli (Australian National University)\n -  - IPAM, 2013\n\n#### Recent Conference Talks\n-  - Jun 2015\n-  - Sep 2014\n-  - Jun 2014\n-  - Dec 2013\n-  - Jul 2013\n-  - Jun 2013\n-  - Oct 2012\n-  - Jun 2012\n-  - Jun 2012\n\n#### 3D Computer Vision\n *  - Steve Seitz (University of Washington) 2011\n *  - Steve Seitz (University of Washington) 2013\n\n#### Internet Vision\n *  - Noah Snavely (Cornell University) 2011\n *  - Noah Snavely (Cornell University) 2014\n *  - Steve Seitz (University of Washington) 2013\n\n#### Computational Photography\n *  - Richard Szeliski (Microsoft Research) 2013\n *  - William T. Freeman (MIT) 2011\n *  -  Yair Weiss (The Hebrew University of Jerusalem) 2011\n *  -  Peyman Milanfar (UC Santa Cruz/Google) 2010\n *  Andrew Blake (Microsoft Research) 2007\n *  - William T. Freeman (MIT) 2012\n *  - Fr\xc3\xa9do Durand (MIT) 2012\n *  - Rich Radke (Rensselaer Polytechnic Institute) 2014\n\n#### Learning and Vision\n *  - William T. Freeman (MIT) 2011\n *  - Simon Lucey (CMU) 2008\n *  - Yair Weiss (The Hebrew University of Jerusalem) 2009\n\n#### Object Recognition\n *  - Larry Zitnick (Microsoft Research)\n *  - Fei-Fei Li (Stanford University)\n\n#### Graphical Models\n *  - Pedro Felzenszwalb (Brown University) 2012\n *  - Zoubin Ghahramani (University of Cambridge) 2009\n *  - Sam Roweis (NYU) 2006\n *  -  Yair Weiss (The Hebrew University of Jerusalem) 2009\n\n#### Machine Learning\n *  - Jeff A. Bilmes (UC Berkeley) 1998\n *  - Christopher Bishop (Microsoft Research) 2009\n *  - Chih-Jen Lin (National Taiwan University) 2006\n *  - Michael I. Jordan (UC Berkeley)\n\n#### Optimization\n *  - Stephen J. Wright (University of Wisconsin-Madison)\n *  - Lieven Vandenberghe (University of California, Los Angeles)\n *  - Andrew Fitzgibbon (Microsoft Research)\n *  - Francis Bach (INRIA)\n *  - Daniel Cremers (Technische Universit\xc3\xa4t M\xc3\xbcnchen) ()\n\n#### Deep Learning\n *  - Geoffrey E. Hinton (University of Toronto)\n *  -  Ruslan Salakhutdinov (University of Toronto)\n *  - Yoshua Bengio (University of Montreal)\n *  -  Alex Krizhevsky (University of Toronto)\n *  Yann LeCun (NYU/Facebook Research) 2014\n *  - Rob Fergus (NYU/Facebook Research)\n *  - St\xc3\xa9phane Mallat (Ecole Normale Superieure)\n *  - IPAM, 2012\n * \n *  - Reykjavik, Iceland 2014\n\t*  - Yoshua Bengio (Universtiy of Montreal)\n\t*  - Yoshua Bengio (University of Montreal)\n\t*  - Yoshua Bengio (University of Montreal)\n\n## Software\n\n#### Annotation tools\n* \n* \n* \n* \n\n#### External Resource Links\n *  - Jia-Bin Huang (UIUC)\n *  - CVPapers\n *  - Xin Li (West Virginia University)\n * \n\n#### General Purpose Computer Vision Library\n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n\n#### Multiple-view Computer Vision\n* \n* \n*  - geometric computer vision algorithms\n*  - Minimal problems solver\n* \n* \n* \n*  - Multiple View Geometry; Structure from Motion library & softwares\n* \n* \n* \n* \n* \n\n\n#### Feature Detection and Extraction\n* \n* \n  * David G. Lowe, ""Distinctive image features from scale-invariant keypoints,"" International Journal of Computer Vision, 60, 2 (2004), pp. 91-110.\n* \n* \n  * Stefan Leutenegger, Margarita Chli and Roland Siegwart, ""BRISK: Binary Robust Invariant Scalable Keypoints"", ICCV 2011\n* \n  * Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, ""SURF: Speeded Up Robust Features"", Computer Vision and Image Understanding (CVIU), Vol. 110, No. 3, pp. 346--359, 2008\n* \n  * A. Alahi, R. Ortiz, and P. Vandergheynst, ""FREAK: Fast Retina Keypoint"", CVPR 2012\n* \n  * Pablo F. Alcantarilla, Adrien Bartoli and Andrew J. Davison, ""KAZE Features"", ECCV 2012\n* \n\n#### High Dynamic Range Imaging\n* \n\n#### Semantic Segmentation\n* \n\n#### Low-level Vision\n\n###### Stereo Vision\n * \n * \n * \n * \n\n###### Optical Flow\n * \n * \n * \n * \n *  - Ce Liu (MIT)\n * \n * \n * \n\n###### Image Denoising\nBM3D, KSVD,\n\n###### Super-resolution\n * \n    * Pickup, L. C. Machine Learning in Multi-frame Image Super-resolution, PhD thesis 2008\n * \n    * W. T Freeman and C. Liu. Markov Random Fields for Super-resolution and Texture Synthesis. In A. Blake, P. Kohli, and C. Rother, eds., Advances in Markov Random Fields for Vision and Image Processing, Chapter 10. MIT Press, 2011\n * \n    * K. I. Kim and Y. Kwon, ""Single-image super-resolution using sparse regression and natural image prior"", IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 32, no. 6, pp. 1127-1133, 2010.\n * \n    * T. Peleg and M. Elad, A Statistical Prediction Model Based on Sparse Representations for Single Image Super-Resolution, IEEE Transactions on Image Processing, Vol. 23, No. 6, Pages 2569-2582, June 2014\n * \n    * R. Zeyde, M. Elad, and M. Protter On Single Image Scale-Up using Sparse-Representations, Curves & Surfaces, Avignon-France, June 24-30, 2010 (appears also in Lecture-Notes-on-Computer-Science - LNCS).\n * \n    * Jianchao Yang, John Wright, Thomas Huang, and Yi Ma. Image super-resolution via sparse representation. IEEE Transactions on Image Processing (TIP), vol. 19, issue 11, 2010.\n * \n    * H. Chang, D.Y. Yeung, Y. Xiong. Super-resolution through neighbor embedding. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), vol.1, pp.275-282, Washington, DC, USA, 27 June - 2 July 2004.\n * \n    *  Yu Zhu, Yanning Zhang and Alan Yuille, Single Image Super-resolution using Deformable Patches, CVPR 2014\n * \n    * Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Learning a Deep Convolutional Network for Image Super-Resolution, in ECCV 2014\n * \n    * R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted Anchored Neighborhood Regression for Fast Super-Resolution, ACCV 2014\n * \n    * Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja, Single Image Super-Resolution using Transformed Self-Exemplars, IEEE Conference on Computer Vision and Pattern Recognition, 2015\n\n###### Image Deblurring\n\nNon-blind deconvolution\n * \n * \n * \n * \n * \n * \n\nBlind deconvolution\n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n\nNon-uniform Deblurring\n * \n * \n * \n * \n\n\n###### Image Completion\n * \n * \n * \n * \n\n###### Image Retargeting\n * \n\n###### Alpha Matting\n * \n * \n * \n * \n * \n\n###### Image Pyramid\n* \n* \n\n###### Edge-preserving image processing\n * \n * \n * \n * \n * \n * \n * \n * \n * \n\n#### Intrinsic Images\n\n* \n* \n\n#### Contour Detection and Image Segmentation\n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n\n#### Interactive Image Segmentation\n * \n * \n * \n * \n * \n * \n\n#### Video Segmentation\n * \n * \n * \n * \n\n#### Camera calibration\n * \n * \n * \n\n#### Simultaneous localization and mapping\n\n###### SLAM community:\n * \n * \n\n###### Tracking/Odometry:\n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n\n###### Graph Optimization:\n *  -- Georgia Institute of Technology\n * \n\n###### Loop Closure:\n *  - also available in \n * \n\n###### Localization & Mapping:\n * \n * \n * \n\n#### Single-view Spatial Understanding\n *  - Derek Hoiem (CMU)\n *  - Varsha Hedau (UIUC)\n *  - David C. Lee (CMU)\n *  - Ruiqi Guo (UIUC)\n\n#### Object Detection\n * \n * \n * \n * \n * \n * \n * \n * \n * \n\n#### Nearest Neighbor Search\n\n###### General purpose nearest neighbor search\n * \n * \n * \n\n###### Nearest Neighbor Field Estimation\n * \n * \n * \n * \n * \n\n#### Visual Tracking\n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n\n#### Saliency Detection\n\n#### Attributes\n\n#### Action Reconition\n\n#### Egocentric cameras\n\n#### Human-in-the-loop systems\n\n#### Image Captioning\n *  -\n\n#### Optimization\n *  - Nonlinear least-square problem and unconstrained optimization solver\n * - Nonlinear least-square problem and unconstrained optimization solver\n *  - Factor graph based discrete optimization and inference solver\n *  - Factor graph based lease-square optimization solver\n\n#### Deep Learning\n * \n\n#### Machine Learning\n * \n * \n * \n\n## Datasets\n\n#### External Dataset Link Collection\n *  - CVPapers\n *  - Which paper provides the best results on standard dataset X?\n * \n * \n * \n * \n * \n * \n * \n\n#### Low-level Vision\n\n###### Stereo Vision\n * \n * \n * \n * \n\n###### Optical Flow\n * \n * \n * \n * \n\n###### Video Object Segmentation\n * \n * \n\n\n###### Change Detection\n * \n * \n\n###### Image Super-resolutions\n * \n\n#### Intrinsic Images\n * \n * \n * \n\n#### Material Recognition\n * \n * \n * \n\n#### Multi-view Reconsturction\n* \n\n#### Saliency Detection\n\n#### Visual Tracking\n * \n * \n * \n * \n * \n\n#### Visual Surveillance\n * \n * \n\n#### Saliency Detection\n\n#### Change detection\n * \n\n#### Visual Recognition\n\n###### Image Classification\n * \n * \n\n###### Self-supervised Learning\n* \n\n###### Scene Recognition\n * \n * \n\n###### Object Detection\n * \n * \n * \n\n###### Semantic labeling\n * \n * \n * \n * \n\n###### Multi-view Object Detection\n * \n * \n * \n * \n * \n * \n\n###### Fine-grained Visual Recognition\n * \n * \n\n###### Pedestrian Detection\n * \n * \n\n#### Action Recognition\n\n###### Image-based\n\n###### Video-based\n * \n * \n\n###### Image Deblurring\n * \n * \n\n#### Image Captioning\n * \n * \n * \n\n#### Scene Understanding\n #  - A RGB-D Scene Understanding Benchmark Suite\n #  - Indoor Segmentation and Support Inference from RGBD Images\n\n#### Aerial images\n #  - Learning Aerial Image Segmentation From Online Maps\n\n\n## Resources for students\n\n#### Resource link collection\n *  - Fr\xc3\xa9do Durand (MIT)\n *  - Aaron Hertzmann (Adobe Research)\n *  - Yashar Ganjali, Aaron Hertzmann (University of Toronto)\n *  - Simon Peyton Jones (Microsoft Research)\n *  - Tao Xie (UIUC) and Yuan Xie (UCSB)\n\n#### Writing\n *  - Fr\xc3\xa9do Durand (MIT)\n *  - Fr\xc3\xa9do Durand (MIT)\n *  - Fr\xc3\xa9do Durand (MIT)\n *  - William T. Freeman (MIT)\n *  - Simon Peyton Jones (Microsoft Research)\n *  - SIGGRAPH ASIA 2011 Course\n *  - Aaron Hertzmann (Adobe Research)\n *  - Jim Blinn\n *  - Jim Kajiya (Microsoft Research)\n *  - Li-Yi Wei (The University of Hong Kong)\n *  - Martin Martin Hering Hering--Bertram (Hochschule Bremen University of Applied Sciences)\n *  - Takeo Igarashi (The University of Tokyo)\n *  - Marc H. Raibert (Boston Dynamics, Inc.)\n *  - Derek Hoiem (UIUC)\n *  - Wojciech Jarosz (Dartmouth College)\n\n\n#### Presentation\n *  - Fr\xc3\xa9do Durand (MIT)\n *  - David Fleet (University of Toronto) and Aaron Hertzmann (Adobe Research)\n *  - Colin Purrington\n\n#### Research\n *  - William T. Freeman (MIT)\n *  - Richard Hamming\n *  - Yi Ma (UIUC)\n *  - Robert L. Park\n *  - Thomas Funkhouser (Cornell University)\n *  - David Chapman (MIT)\n *  - Ming-Hsuan Yang (UC Merced)\n *  - Jia-Bin Huang (UIUC)\n *  - Jia-Bin Huang (UIUC)\n\n#### Time Management\n *  - Randy Pausch (CMU)\n\n## Blogs\n *  - Satya Mallick\n *  - Tomasz Malisiewicz\n *  - Vincent Spruyt\n *  - Andrej Karpathy\n *  - Utkarsh Sinha\n *  - Eugene Khvedchenya\n *  - Jason Chin (University of Western Ontario)\n\n\n## Links\n*  - David Lowe\n* \n* \n* \n* \n* \n*\n## Songs\n* \n* \n* \n\n## Licenses\nLicense\n\n\n\nTo the extent possible under law,  has waived all copyright and related or neighboring rights to this work.\n'"
"SpecuCheck is a Windows utility for checking the state of the software mitigations and hardware against  CVE-2017-5754 (Meltdown), CVE-2017-5715 (Spectre v2), CVE-2018-3260 (Foreshadow), and CVE-2018-3639 (Spectre v4)","b""# SpecuCheck\n\nSpecuCheck is a Windows utility for checking the state of the software and hardware mitigations against CVE-2017-5754 (Meltdown), CVE-2017-5715 (Spectre v2), CVE-2018-3260 (Foreshadow), and CVE-2018-3639 (Spectre v4). It uses two new information classes that were added to the NtQuerySystemInformation API call as part of the recent patches introduced in January 2018 and reports the data as seen by the Windows Kernel. \n\nAn  Microsoft Powershell Cmdlet Module now exists as well, which is the recommended and supported way to get this information.\n\n## Screenshots\n\n\n\n## Introduction\n\nOn January 3rd 2018, Intel, AMD and ARM Holdings, as well as a number of OS Vendors reported a series of vulnerabilities that were discovered by Google Project Zero:\n\n* Variant 1: bounds check bypass (CVE-2017-5753)\n* Variant 2: branch target injection (CVE-2017-5715)\n* Variant 3: rogue data cache load (CVE-2017-5754)\n\nMicrosoft released patches for Windows 7 SP1 and higher later that same day. These patches, depending on architecture, OS version, boot settings and a number of hardware-related properties, apply a number of software and hardware mitigations against these issues. The enablement state of these mitigations, their availability, and configuration is stored by the Windows kernel in a number of global variables, and exposed to user-mode callers through an undocumented system call.\n\nAdditionally, new side channel attacks were reported, such as Spectre Variant 4: speculative store bypass (CVE-2018-3639) and Foreshadow: L1 terminal fault (CVE-2018-3620) which were fixed in Windows 7 SP1 and higher with patches in August's Patch Tuesday.\n\nSpecuCheck takes advantage of this system call in order to confirm if a system has indeed been patched (non-patched systems will fail the call) and what the status of the mitigations are, which can be used to determine potential performance pitfalls.\n\n## Motivation\n\nThere was originally a lot of noise, hype, and marketing around the issue, and not a lot of documentation on how to see if you were affected, and at what performance overhead. SpecuCheck aimed to make that data easily accessible by users and IT departments, to avoid having to use a Windows debugger or reverse engineer the API themselves.\n\nSince then, Microsoft has done great work to expose that data from the kernel-mode in a concise matter, which succinctly indicates the kernel's support and usage of the various mitigating technologies and hardware features, and released a PowerShell CmdLet Module to retrieve that data. SpecuCheck, therefore, remains only as a research tool and is not recommended -- please use the Microsoft-approved PowerShell Module instead.\n\n## Installation on Windows\n\nTo run SpecuCheck, simply execute it on the command-line:\n\n\n\nWhich will result in an informational screen indicating which features/mitigations are enabled. If you see the text:\n\n\n\nThis indicates that your system is not currently patched to mitigate against these vulnerabilities.\n\n## References\n\nIf you would like to know more about my research or work, I invite you to check out my blog at  as well as my training & consulting company, Winsider Seminars & Solutions Inc., at .\n\nYou should also definitely read the incredibly informative .\n\nFor additional information on the appropriate and required Windows patches, please read the  and additional  .\n\n## Caveats\n\nSpecuCheck relies on undocumented system calls and information classes which are subject to change. Additionally, SpecuCheck only returns the information that the Windows Kernel is storing about the state of the mitigations and hardware features -- based on policy settings (registry, boot parameters) or other compatibility flags, the Windows Kernel's state may not match the true hardware state. The goal of this tool is to give you a Windows-specific assessment, not a hardware assessment that is OS-agnostic.\n\nSpecuCheck is only a research tool and is not recommended for general or commercial use -- please use the Microsoft-approved PowerShell Module instead.\n\n## License\n\n\n"""
CLI for the Apple Dev Center,"b'\n\nNote: Cupertino stopped working due to a recent change on the Apple Developer Portal. A maintained alternative is to use  to communicate with Apples Developer back-end, or use any of the , like  or .\n\n------\n\nAutomate administrative tasks that you would normally have to do through the Apple Dev Center websites. Lifes too short to manage device identifiers by hand!\n\n> Cupertino is named after : home to Apple, Inc.s world headquarters.\n> Its part of a series of world-class command-line utilities for iOS development, which includes  (Building & Distribution),  (Push Notifications),  (In-App Purchase Receipt Verification),  (Passbook pass generation), and  (iTunes Store API).\n\n## Requirements\n\nCupertino requires the , which can be installed with the following command:\n\n\n\n## Installation\n\n\n\n## Usage\n\n### Authentication\n\n\n\n_Credentials are saved in the Keychain. You will not be prompted for your username or password by commands while you are logged in. (Mac only)\n\nAlternatively, username and password can also be provided by setting the\n\n\n\n\n\n\n_Adds (without an editor) a list of devices to a provisioning profile_\n\n\n\n_Removes (without an editor) a list of devices from a provisioning profile_\n\n---\n\n\n\n### App IDs\n\n\n\n\n\n### Certificates\n\n\n\n\n\n## CSV Output\n\nThe following commands will format their output as  when the  argument is passed:\n\n- \n- \n- \n- \n\n## Commands\n\n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n\n## Options\n\nGlobal options:\n\n-  : Username\n-  : Password\n-  : Team Identifier\n-  : Set log level to INFO\n-  : Set log level to DEBUG\n-  : Output options are  or  \n\nSpecific options for certain commands:\n\n-  : Options are  or \n\n## Proxies\n\nCupertino will access the provisioning portal through a proxy if the  environment variable is set, with optional credentials  and .\n\n## License\n\nCupertino is released under an MIT license. See LICENSE for more information.\n'"
Community-based GPL-licensed network monitoring system,"b'\n\nIntroduction\n------------\n\nLibreNMS is an auto-discovering PHP/MySQL/SNMP based network monitoring\nwhich includes support for a wide range of network hardware and operating\nsystems including Cisco, Linux, FreeBSD, Juniper, Brocade, Foundry, HP and\nmany more.\n\nWe intend LibreNMS to be a viable project and community that:\n- encourages contribution,\n- focuses on the needs of its users, and\n- offers a welcoming, friendly environment for everyone.\n\nThe [Debian Social Contract][10] will be the basis of our priority system,\nand mutual respect is the basis of our behavior towards others.\n\n\nDocumentation\n-------------\n\nDocumentation can be found in the [doc directory][5] or [docs.librenms.org][16], including instructions\nfor installing and contributing.\n\n\nParticipating\n-------------\n\nYou can participate in the project by:\n- Talking to us on [Discord][4] or [Twitter][3].\n- Joining the \n- Improving the [documentation][5].\n- Cloning the [repository][2] and filing [pull requests][19] on GitHub.\n-  on our Community Forums\n- See [CONTRIBUTING][15] for more details.\n\n\nVM image\n--------\n\nYou can try LibreNMS by downloading a VM image.  Currently, a Ubuntu-based\nimage is supplied and has been tested with [VirtualBox][8].\n\nDownload one of the [VirtualBox images][11] we have available, documentation is provided which details\nlogin credentials and setup details.\n\nLicense\n-------\n\nCopyright (C) 2006-2012 Adam Armstrong \n\nCopyright (C) 2013-2023 by individual LibreNMS contributors\n\n This program is free software: you can redistribute it and/or modify\n it under the terms of the GNU General Public License as published by\n the Free Software Foundation, either version 3 of the License, or\n (at your option) any later version.\n\n This program is distributed in the hope that it will be useful,\n but WITHOUT ANY WARRANTY; without even the implied warranty of\n MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n GNU General Public License for more details.\n\n You should have received a copy of the GNU General Public License\n along with this program.  If not, see .\n\n[LICENSE.txt][14] contains a copy of the full GPLv3 licensing conditions.\n\nThe following additional license conditions apply to LibreNMS (a GPL\nexception):\n\n  As a special exception, you have permission to link or otherwise combine\n  LibreNMS with the included copies of the following third-party software,\n  and distribute modified versions, as long as you follow the requirements\n  of the GNU GPL v3 in regard to all of the remaining software (comprising\n  LibreNMS).\n\n  Please see [Acknowledgements][17]\n\n[2]: https://github.com/librenms/librenms ""Main LibreNMS GitHub repo""\n[3]: https://twitter.com/librenms ""@LibreNMS on Twitter""\n[4]: https://discord.gg/librenms ""Discord LibreNMS Server""\n[5]: https://github.com/librenms/librenms/tree/master/doc/\n[8]: https://www.virtualbox.org/ ""VirtualBox""\n[10]: http://www.debian.org/social_contract ""Debian project social contract""\n[11]: https://www.librenms.org/#downloads\n[14]: https://github.com/librenms/librenms/tree/master/LICENSE.txt\n[15]: https://docs.librenms.org/General/Contributing/\n[16]: https://docs.librenms.org/\n[17]: https://docs.librenms.org/General/Acknowledgement/\n[19]: https://github.com/librenms/librenms/pulls\n\n\n## Backers\n\nSupport us with a monthly donation and help us continue our activities. []\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Sponsors\n\nBecome a sponsor and get your logo on our README on GitHub with a link to your site. []\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
A flexible framework of neural networks for deep learning,"b'[<marko.inline.RawText object at 0x000001F254324CD0>, <marko.inline.Link object at 0x000001F254327B10>, <marko.inline.RawText object at 0x000001F254324050>]\n\n----\n\n\n\n# Chainer: A deep learning framework\n\n\n\n\n\n\n\n\n\n| \n| \n| Tutorials ()\n| Examples (, )\n| \n| \n\nForum (, )\n| Slack invitation (, )\n| Twitter (, )\n\nChainer is a Python-based deep learning framework aiming at flexibility.\nIt provides automatic differentiation APIs based on the define-by-run approach (a.k.a. dynamic computational graphs) as well as object-oriented high-level APIs to build and train neural networks.\nIt also supports CUDA/cuDNN using  for high performance training and inference.\nFor more details about Chainer, see the documents and resources listed above and join the community in Forum, Slack, and Twitter.\n\n## Installation\n\nFor more details, see the \n\nTo install Chainer, use .\n\n\n\nTo enable CUDA support,  is required.\nRefer to the .\n\n\n## Docker image\n\nWe are providing the official Docker image.\nThis image supports .\nLogin to the environment with the following command, and run the Python interpreter to use Chainer with CUDA and cuDNN support.\n\n\n\n\n## Contribution\n\nSee the .\n\n\n## ChainerX\n\nSee the .\n\n\n## License\n\nMIT License (see  file).\n\n\n## More information\n\n- \n\n## References\n\nTokui, Seiya, et al. ""Chainer: A Deep Learning Framework for Accelerating the Research Cycle."" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2019.\n \n\nTokui, S., Oono, K., Hido, S. and Clayton, J.,\nChainer: a Next-Generation Open Source Framework for Deep Learning,\nProceedings of Workshop on Machine Learning Systems(LearningSys) in\nThe Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS), (2015)\n, \n\nAkiba, T., Fukuda, K. and Suzuki, S.,\nChainerMN: Scalable Distributed Deep Learning Framework,\nProceedings of Workshop on ML Systems in\nThe Thirty-first Annual Conference on Neural Information Processing Systems (NIPS), (2017)\n, \n'"
Easily craft fast Neural Networks on iOS! Use TensorFlow models. Metal under the hood.,"b'# Bender\n\n\n\n\n\n\n\n\n\n\n\n\n\nBender is an abstraction layer over MetalPerformanceShaders useful for working with neural networks.\n\n## Contents\n* \n* \n* \n* \n* \n* \n* \n* \n\nThe documentation can be found under the  folder:\n*  contains the most important information to get started.\n* [Supported Layers] explains which layers are supported and how they map to TensorFlow ops.\n* [Importing] explains how to import models from other frameworks such as TensorFlow. You can also find information on how to enhance this functionality for custom implementations.\n\n## Introduction\n\nBender is an abstraction layer over MetalPerformanceShaders which is used to work with neural networks. It is of growing interest in the AI environment to execute neural networks on mobile devices even if the training process has been done previously. We want to make it easier for everyone to execute pretrained networks on iOS.\n\nBender allows you to easily define and run neural networks using the most common layers like Convolution, Pooling, FullyConnected and some normalizations among others. It is also flexible in the way it receives the parameters for these layers.\n\nWe also want to support loading models trained on other frameworks such as TensorFlow or Caffe2. Currently Bender includes an adapter for TensorFlow that loads a graph with variables and ""translates"" it to Bender layers. This feature supports a subset of TensorFlows operations but we plan to enhance it to cover more cases.\n\n## Why did we need Bender? \n\nAt [Xmartlabs] we were about to start a Machine Learning project and investigated frameworks to use in iOS. We found MetalPerformanceShaders useful but not very user friendly and we saw ourselves repeating a lot of code and information. That is why we starting building a framework to handle that kind of stuff.\n\nWe also found ourselves creating scripts to translate the models we had from training with TensorFlow to iOS. This means transposing the weights to the MPSCNN format and also mapping the parameters of the different kinds of layers in TensorFlow to the parameters used by the MPSCNN kernels. TensorFlow can be compiled for iOS but currently it does not support running on GPU which we wanted to do. We also did not want to include TensorFlows static library into our project. This is why we also started to work on an adapter that would parse a TF graph and translate it to our Bender layers.\n\n## Usage\n\nYou can define your own network in Bender using our custom operator or you can load a model exported from TensorFlow. Defining a network and loading a model can be done like this:\n\n\n\nYou can read more information about this in .\n\nIf you want to define your network yourself you can do it like this:\n\n\n\nand once youre done with all your layers:\n\n\n\nTo know more about this have a look at .\n\n\n## Requirements\n\n* Xcode 9\n* iOS 11.0+ (but deployment target is iOS 10.0, so iOS 10 is supported)\n\n## Getting involved\n\n* If you want to contribute please feel free to submit pull requests.\n* If you have a feature request please open an issue.\n* If you found a bug or need help please check older issues, .\n\nBefore contribute check the [CONTRIBUTING] file for more info.\n\nIf you use Bender in your app We would love to hear about it! Drop us a line on .\n\n## Examples\n\nFollow these steps to run the examples:\n* Clone Bender repository (or download it).\n* Run  in the downloaded folder.\n* Open Bender workspace and run the Example project.\n\n> There is an Image recognition example which includes a MobileNet model in Bender and one in CoreML. It is also set up to run an Inception model but you will have to download it separately as it is almost 100 MB in size.\nYou can download it from http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz but then you have to freeze it and add it to the Example Xcode project as inception_v3.pb.\n\n## Installation\n\n#### CocoaPods\n\nTo install Bender, simply add the following line to your Podfile:\n\n\n\n> Remember that Bender compiles for iOS 10. So you must add  to your Podfile\n\n#### Carthage\n\n is a simple, decentralized dependency manager for Cocoa.\n\nTo install Bender, add the following line to your Cartfile:\n\n\n\nThen run:\n\n\n\nFinally, drag the built  binaries for ,  and  to your applications Xcode project.\n\n## Author\n\n*  ()\n\n\n# Change Log\n\nThis can be found in the  file.\n\n\n[Xmartlabs]: http://xmartlabs.com\n[Importing]: Documentation/Importing.md\n[CONTRIBUTING]: .github/CONTRIBUTING.md\n[API]: Documentation/API.md\n[Supported Layers]: Documentation/Supported_Layers.md\n\n\n## License\n\n\n## Citation\nIf you use this code in your research please cite us:\n\n\n'"
markdown preview plugin for (neo)vim,"b' \xe2\x9c\xa8 Markdown Preview for (Neo)vim \xe2\x9c\xa8 \n\n> Powered by \xe2\x9d\xa4\xef\xb8\x8f\n\n### Introduction\n\n> It only works on Vim >= 8.1 and Neovim\n\nPreview Markdown in your modern browser with synchronised scrolling and flexible configuration.\n\nMain features:\n\n- Cross platform (MacOS/Linux/Windows)\n- Synchronised scrolling\n- Fast asynchronous updates\n-  for typesetting of math\n- \n- \n- \n- \n- \n- \n- \n- Emojis\n- Task lists\n- Local images\n- Flexible configuration\n\nNote the plugin  is not needed for typesetting math.\n\n\n\n### Installation & Usage\n\nInstall with :\n\n\n\nOr install with :\n\n\n\nOr with :\n\n\n\nOr with :\n\nPlace this in your  or ,\n\n... then run the following in Vim (to complete the  installation):\n\nOr with :\n\nAdd this in your \n\n\nOr with :\n\nAdd this in your \n\n\n\nOr by hand:\n\n\n\nadd plugin to the  directory:\n\n\n\nPlease make sure that you have installed  and .\nOpen  and run  to make it workable\n\n### MarkdownPreview Config:\n\n\n\nMappings:\n\n\n\nCommands:\n\n\n\n### Custom Examples\n\nTable of contents\n\n> one of\n\n    ${toc}\n    [[toc]]\n    [toc]\n    [[toc]]\n\nImage Size:\n\n\n\nPlantUML:\n\n    @startuml\n    Bob -> Alice : hello\n    @enduml\n\nOr\n\n    \n\nKaTeX:\n\n    $sqrt{3x-1}+(1+x)^2$\n\n    $$begin{array}{c}\n\n    nabla times vec{mathbf{B}} -, frac1c, frac{partialvec{mathbf{E}}}{partial t} &\n    = frac{4pi}{c}vec{mathbf{j}}    nabla cdot vec{mathbf{E}} & = 4 pi rho \n\n    nabla times vec{mathbf{E}}, +, frac1c, frac{partialvec{mathbf{B}}}{partial t} & = vec{mathbf{0}} \n\n    nabla cdot vec{mathbf{B}} & = 0\n\n    end{array}$$\n\nmermaid:\n\n    \n\njs-sequence-diagrams:\n\n    \nFlowchart:\n\n    \n\ndot:\n\n    \n\nchart:\n\n    \n\n### FAQ\n\n#### Why is the synchronised scrolling lagging?\n\nSet  to a small number, for instance: \n\nWSL 2 issue: Can not open browser when using WSL 2 with terminal Vim.\n\n> if you are using Ubuntu you can install xdg-utils using \n> checkout  for more detail.\n\n#### How can I change the dark/light theme?\n\nThe default theme is based on your system preferences.\nThere is a button hidden in the header to change the theme. Place your mouse over the header to reveal it.\n\n#### How can I pass CLI options to the browser, like opening in a new window?\n\nAnswer: Add the following to your Neovim init script:\n\nLinux\n\nReplace  with  if you prefer. Both browsers recognize the  option.\n\nmacOS\n\nReplace  with  or  if you prefer. They all recognize the  option.\n\n### About Vim Support\n\nVim support is powered by \n\n### References\n\n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n\n### Buy Me A Coffee \xe2\x98\x95\xef\xb8\x8f\n\n\n\n\n'"
"A chat server with OAuth2 authentication, persistent and searchable history, video and audio, markdown formatting, private and public rooms, stars, votes, embedded games, and many other features","b""\n\n\nA chat server with authentication, persistent and searchable history, rich markdown formatting, video, private rooms, conversation highlighting, plugins, persisted notifications, code and table rendering, specialized link boxing, github hooks, bots, and many other features.\n\n\n\n\n\n\n****\n\nYou can see it in action or use it on https://miaou.dystroy.org (anybody can create a room for public or private use on this server).\n\n# Installing a server\n\nIf you want to install Miaou, the installation documentation is available .\n\nDon't hesitate to come ask us some advices in one of the dedicated chat rooms  and  (please note that they're currently more active between 8 and 19 GMT).\n\nAnd if you run your own server, please tell us, we're always happy to learn about installations.\n\n# Contributing\n\nAs described in , Miaou is mostly coded in JavaScript.\n\nStuff includes node, PostgreSQL, OAuth2, socket.io, WebRTC, express, Bluebird, Redis, Pug, Passport.js, hu.js, jQuery, sass/scss, Uglify-js, gulp, jest, travis-ci, and nginx.\n\nMany features are implemented as plugins, and that's where you should look first: .\n\nIf you have the ability and will to contribute, come and discuss with us. The best landing place is usually the  where you can ping @dystroy or @Florian.\n\nHelp is welcome but remember:\n\n1. Come and discuss with us before to code\n2. And, always test yourself and run the test suite before doing a pull request.\n\n## License\n\nMost of Miaou follows the . Exceptions are specified .\n\nCopyright (c) 2014 Denys S\xc3\xa9guret <>\n"""