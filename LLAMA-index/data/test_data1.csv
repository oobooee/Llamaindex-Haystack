description,readme
Bolts is a collection of low-level libraries designed to make developing mobile apps easier.,"b'Bolts\n============\n\n\n\n\n\n\n\n\n\nBolts is a collection of low-level libraries designed to make developing mobile\napps easier. Bolts was designed by Parse and Facebook for our own internal use,\nand we have decided to open source these libraries to make them available to\nothers. Using these libraries does not require using any Parse services. Nor\ndo they require having a Parse or Facebook developer account.\n\nBolts includes:\n\n* ""Tasks"", which make organization of complex asynchronous code more manageable. A task is kind of like a JavaScript Promise, but available for iOS and Android.\n* An implementation of the , helping you link to content in other apps and handle incoming deep-links.\n\nFor more information, see the .\n\n# Tasks\n\nTo build a truly responsive iOS application, you must keep long-running operations off of the UI thread, and be careful to avoid blocking anything the UI thread might be waiting on. This means you will need to execute various operations in the background. To make this easier, weve added a class called . A task represents the result of an asynchronous operation. Typically, a  is returned from an asynchronous function and gives the ability to continue processing the result of the task. When a task is returned from a function, its already begun doing its job. A task is not tied to a particular threading model: it represents the work being done, not where it is executing. Tasks have many advantages over other methods of asynchronous programming, such as callbacks.  is not a replacement for  or GCD. In fact, they play well together. But tasks do fill in some gaps that those technologies dont address.\n*  takes care of managing dependencies for you. Unlike using  for dependency management, you dont have to declare all dependencies before starting a . For example, imagine you need to save a set of objects and each one may or may not require saving child objects. With an , you would normally have to create operations for each of the child saves ahead of time. But you dont always know before you start the work whether thats going to be necessary. That can make managing dependencies with  very painful. Even in the best case, you have to create your dependencies before the operations that depend on them, which results in code that appears in a different order than it executes. With , you can decide during your operations work whether there will be subtasks and return the other task in just those cases.\n*  release their dependencies.  strongly retains its dependencies, so if you have a queue of ordered operations and sequence them using dependencies, you have a leak, because every operation gets retained forever.  release their callbacks as soon as they are run, so everything cleans up after itself. This can reduce memory use, and simplify memory management.\n*  keep track of the state of finished tasks: It tracks whether there was a returned value, the task was cancelled, or if an error occurred. It also has convenience methods for propagating errors. With , you have to build all of this stuff yourself.\n*  dont depend on any particular threading model. So its easy to have some tasks perform their work with an operation queue, while others perform work using blocks with GCD. These tasks can depend on each other seamlessly.\n* Performing several tasks in a row will not create nested ""pyramid"" code as you would get when using only callbacks.\n*  are fully composable, allowing you to perform branching, parallelism, and complex error handling, without the spaghetti code of having many named callbacks.\n* You can arrange task-based code in the order that it executes, rather than having to split your logic across scattered callback functions.\n\nFor the examples in this doc, assume there are async versions of some common Parse methods, called  and  which return a . In a later section, well show how to define these functions yourself.\n\n## The  Method\n\nEvery  has a method named  which takes a continuation block. A continuation is a block that will be executed when the task is complete. You can then inspect the task to check if it was successful and to get its result.\n\n\n\n\n\nBFTasks use Objective-C blocks, so the syntax should be pretty straightforward. Lets look closer at the types involved with an example.\n\n\n\n\n\nIn many cases, you only want to do more work if the previous task was successful, and propagate any errors or cancellations to be dealt with later. To do this, use the  method instead of .\n\n\n\n\n\n## Chaining Tasks Together\n\nBFTasks are a little bit magical, in that they let you chain them without nesting. If you return a BFTask from , then the task returned by  will not be considered finished until the new task returned from the new continuation block. This lets you perform multiple actions without incurring the pyramid code you would get with callbacks. Likewise, you can return a  from . So, return a  to do more asynchronous work.\n\n\n\n\n\n## Error Handling\n\nBy carefully choosing whether to call  or , you can control how errors are propagated in your application. Using  lets you handle errors by transforming them or dealing with them. You can think of failed tasks kind of like throwing an exception. In fact, if you throw an exception inside a continuation, the resulting task will be faulted with that exception.\n\n\n\n\n\nIts often convenient to have a long chain of success callbacks with only one error handler at the end.\n\n## Creating Tasks\n\nWhen youre getting started, you can just use the tasks returned from methods like  or . However, for more advanced scenarios, you may want to make your own tasks. To do that, you create a . This object will let you create a new , and control whether it gets marked as finished or cancelled. After you create a , youll need to call , , or  to trigger its continuations.\n\n\n\n\n\nIf you know the result of a task at the time it is created, there are some convenience methods you can use.\n\n\n\n\n\n## Creating Async Methods\n\nWith these tools, its easy to make your own asynchronous functions that return tasks. For example, you can make a task-based version of  easily.\n\n\n\n\n\nIts similarly easy to create ,  or .\n\n## Tasks in Series\n\n are convenient when you want to do a series of tasks in a row, each one waiting for the previous to finish. For example, imagine you want to delete all of the comments on your blog.\n\n\n\n\n\n## Tasks in Parallel\n\nYou can also perform several tasks in parallel, using the  method. You can start multiple operations at once, and use  to create a new task that will be marked as completed when all of its input tasks are completed. The new task will be successful only if all of the passed-in tasks succeed. Performing operations in parallel will be faster than doing them serially, but may consume more system resources and bandwidth.\n\n\n\n\n\n## Task Executors\n\nBoth  and  methods have another form that takes an instance of . These are  and . These methods allow you to control how the continuation is executed. The default executor will dispatch to GCD, but you can provide your own executor to schedule work onto a different thread. For example, if you want to continue with work on the UI thread:\n\n\n\nFor common cases, such as dispatching on the main thread, we have provided default implementations of . These include , , , , and . For example:\n\n\n\n## Task Cancellation\n\nIts generally bad design to keep track of the  for cancellation. A better model is to create a ""cancellation token"" at the top level, and pass that to each async function that you want to be part of the same ""cancelable operation"". Then, in your continuation blocks, you can check whether the cancellation token has been cancelled and bail out early by returning a . For example:\n\n\n\nNote: The cancellation token implementation should be thread-safe.\nWe are likely to add some concept like this to Bolts at some point in the future.\n\n# App Links\n\n provide a cross-platform mechanism that allows a developer to define and publish a deep-linking scheme for their content, allowing other apps to link directly to an experience optimized for the device they are running on. Whether you are building an app that receives incoming links or one that may link out to other apps content, Bolts provides tools to simplify implementation of the .\n\n## Handling an App Link\n\nThe most common case will be making your app receive App Links. In-linking will allow your users to quickly access the richest, most native-feeling presentation of linked content on their devices. Bolts makes it easy to handle an inbound App Link (as well as general inbound deep-links) by providing utilities for processing an incoming URL.\n\nFor example, you can use the  utility class to parse an incoming URL in your :\n\n\n\n## Navigating to a URL\n\nFollowing an App Link allows your app to provide the best user experience (as defined by the receiving app) when a user navigates to a link. Bolts makes this process simple, automating the steps required to follow a link:\n\n1. Resolve the App Link by getting the App Link metadata from the HTML at the URL specified.\n2. Step through App Link targets relevant to the device being used, checking whether the app that can handle the target is present on the device.\n3. If an app is present, build a URL with the appropriate al_applink_data specified and navigate to that URL.\n4. Otherwise, open the browser with the original URL specified.\n\nIn the simplest case, it takes just one line of code to navigate to a URL that may have an App Link:\n\n\n\n### Adding App and Navigation Data\n\nUnder most circumstances, the data that will need to be passed along to an app during a navigation will be contained in the URL itself, so that whether or not the app is actually installed on the device, users are taken to the correct content. Occasionally, however, apps will want to pass along data that is relevant for app-to-app navigation, or will want to augment the App Link protocol with information that might be used by the app to adjust how the app should behave (e.g. showing a link back to the referring app).\n\nIf you want to take advantage of these features, you can break apart the navigation process. First, you must have an App Link to which you wish to navigate:\n\n\n\nThen, you can build an App Link request with any additional data you would like and navigate:\n\n\n\n### Resolving App Link Metadata\n\nBolts allows for custom App Link resolution, which may be used as a performance optimization (e.g. caching the metadata) or as a mechanism to allow developers to use a centralized index for obtaining App Link metadata. A custom App Link resolver just needs to be able to take a URL and return a  containing the ordered list of s that are applicable for this device. Bolts provides one of these out of the box that performs this resolution on the device using a hidden ~~UIWebView~~ WKWebview.\n\nYou can use any resolver that implements the  protocol by using one of the overloads on :\n\n\n\nAlternatively, a you can swap out the default resolver to be used by the built-in APIs:\n\n\n\n## App Link Return-to-Referer View\n\nWhen an application is opened via an App Link, a banner allowing the user to ""Touch to return to "" should be displayed. The  provides this functionality. It will take an incoming App Link and parse the referer information to display the appropriate calling app name.\n\n\n\nThe following code assumes that the view controller has an   property that has already been populated with the URL used to open the app. You can then do something like this to show the view:\n\n\n\nIn a navigation-controller view hierarchy, the banner should be displayed above the navigation bar, and  provides an  method to assist with this.\n\n## Analytics\n\nBolts introduces Measurement Event. App Links posts three different Measurement Event notifications to the application, which can be caught and integrated with existing analytics components in your application.\n\n*   \xe2\x80\x94 Raised when your app switches out to an App Links URL.\n*   \xe2\x80\x94 Raised when your app opens an incoming App Links URL.\n*   \xe2\x80\x94 Raised when your app returns back the referrer app using the built-in top navigation back bar view.\n\n### Listen for App Links Measurement Events\n\nThere are other analytics tools that are integrated with Bolts App Links events, but you can also listen for these events yourself:\n\n\n\n### App Links Event Fields\n\nApp Links Measurement Events sends additional information from App Links Intents in flattened string key value pairs. Here are some of the useful fields for the three events.\n\n* \n  * : the URL that opens the app.\n  * : the scheme of .\n  * : the URL that the referrer app added into : .\n  * : the app name that the referrer app added to : .\n  * : the bundle of referrer application.\n  * : the  field in .\n  * : App Links API  version.\n\n*  / \n  * : the URL used to open the other app (or browser). If there is an eligible app to open, this will be the custom scheme url/intent in .\n  * : the scheme of .\n  * : the URL of the page hosting App Links meta tags.\n  * : the hostname of .\n  * :  to indicate success in opening the App Link in another app or browser;  to indicate failure to open the App Link.\n  * :  for open in app,  for open in browser;  when the success field is .\n  * : App Links API version.\n\n# Installation\n\nYou can download the latest framework files from our .\n\nBolts is also available through . To install it simply add the following line to your Podfile:\n\n    pod Bolts\n'"
A curated list of awesome computer vision resources,"b'\n# Awesome Computer Vision: \nA curated list of awesome computer vision resources, inspired by .\n\nFor a list people in computer vision listed with their academic genealogy, please visit \n\n## Contributing\nPlease feel free to send me  or email (jbhuang@vt.edu) to add links.\n\n## Table of Contents\n\n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n\n## Awesome Lists\n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n - \n \n\n## Books\n\n#### Computer Vision\n*  - Simon J. D. Prince 2012\n*  - Rick Szeliski 2010\n*  - David Forsyth and Jean Ponce 2011\n*  - Richard Hartley and Andrew Zisserman 2004\n*  - Linda G. Shapiro 2001\n*  - Stephen E. Palmer 1999\n*  - Kristen Grauman and Bastian Leibe 2011\n*  - Richard J. Radke, 2012\n*  - Reinhard, E., Heidrich, W., Debevec, P., Pattanaik, S., Ward, G., Myszkowski, K 2010\n*  - Justin Solomon 2015\n*  - Stan Birchfield 2018\n*  - Silvio Savarese 2018\n\n#### OpenCV Programming\n*  - Gary Bradski and Adrian Kaehler\n*  - Adrian Rosebrock\n*  - Oscar Deniz Suarez, M\xc2\xaa del Milagro Fernandez Carrobles, Noelia Vallez Enano, Gloria Bueno Garcia, Ismael Serrano Gracia\n\n#### Machine Learning\n*  - Christopher M. Bishop 2007\n*  - Christopher M. Bishop 1995\n*  - Daphne Koller and Nir Friedman 2009\n*  - Peter E. Hart, David G. Stork, and Richard O. Duda 2000\n*  - Tom M. Mitchell 1997\n*  - Carl Edward Rasmussen and Christopher K. I. Williams 2005\n* - Yaser S. Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin 2012\n*  - Michael Nielsen 2014\n*  - David Barber, Cambridge University Press, 2012\n\n#### Fundamentals\n *  - Gilbert Strang 1995\n\n## Courses\n\n#### Computer Vision\n *  - William Hoff (Colorado School of Mines)\n *  - Alexei A. Efros and Trevor Darrell (UC Berkeley)\n *  - Steve Seitz (University of Washington)\n * Visual Recognition ,  - Kristen Grauman (UT Austin)\n *  - Tamara Berg (UNC Chapel Hill)\n *  - Fei-Fei Li and Andrej Karpathy (Stanford University)\n *  - Rob Fergus (NYU)\n *  - Derek Hoiem (UIUC)\n *  - Kalanit Grill-Spector and Fei-Fei Li (Stanford University)\n *  - Fei-Fei Li (Stanford University)\n *  - Antonio Torralba and Bill Freeman (MIT)\n *  - Bastian Leibe (RWTH Aachen University)\n *  - Bastian Leibe (RWTH Aachen University)\n *  Pascal Fua (EPFL):\n *  Carsten Rother (TU Dresden):\n *  Carsten Rother (TU Dresden):\n *  Daniel Cremers (TU Munich):\n\n\n\n\n#### Computational Photography\n*  - Alexei A. Efros (UC Berkeley)\n*  - Alexei A. Efros (CMU)\n*  - Derek Hoiem (UIUC)\n*  - James Hays (Brown University)\n*  - Fredo Durand (MIT)\n*  - Ramesh Raskar (MIT Media Lab)\n*  - Irfan Essa (Georgia Tech)\n*  - Stanford University\n*  - Rob Fergus (NYU)\n*  - Kyros Kutulakos (University of Toronto)\n*  - Kyros Kutulakos (University of Toronto)\n*  - Rich Radke (Rensselaer Polytechnic Institute)\n*  - Rich Radke (Rensselaer Polytechnic Institute)\n\n#### Machine Learning and Statistical Learning\n *  - Andrew Ng (Stanford University)\n *  - Yaser S. Abu-Mostafa (Caltech)\n *  - Trevor Hastie and Rob Tibshirani (Stanford University)\n *  - Tomaso Poggio, Lorenzo Rosasco, Carlo Ciliberto, Charlie Frogner, Georgios Evangelopoulos, Ben Deen (MIT)\n *  - Genevera Allen (Rice University)\n *  - Michael Jordan (UC Berkeley)\n *  - David MacKay (University of Cambridge)\n *  - Lester Mackey (Stanford)\n *  - Andrew Zisserman (University of Oxford)\n *  - Sebastian Thrun (Stanford University)\n *  - Charles Isbell, Michael Littman (Georgia Tech)\n *  - Fei-Fei Li, Andrej Karphaty, Justin Johnson (Stanford University)\n *  - Rudolph Triebel (TU Munich)\n\n\n\n#### Optimization\n *  - Stephen Boyd (Stanford University)\n *  - Stephen Boyd (Stanford University)\n *  - Stephen Boyd (Stanford University)\n *  - (MIT)\n *  - Ryan Tibshirani (CMU)\n\n## Papers\n\n#### Conference papers on the web\n *  - Computer vision papers on the web\n *  - Graphics papers on the web\n *  - NIPS papers on the web\n * \n *  - Keith Price (USC)\n *  - (USC)\n\n#### Survey Papers\n * \n * \n * \n\n ## Pre-trained Computer Vision Models\n *  These models are trained on custom objects\n\n## Tutorials and talks\n\n#### Computer Vision\n *  - Lectures, keynotes, panel discussions on computer vision\n *  - Jitendra Malik (UC Berkeley) 2013\n *  - Andrew Blake (Microsoft Research) 2008\n *  - Jitendra Malik (UC Berkeley) 2008\n *  - Fatih Porikli (Australian National University)\n -  - IPAM, 2013\n\n#### Recent Conference Talks\n-  - Jun 2015\n-  - Sep 2014\n-  - Jun 2014\n-  - Dec 2013\n-  - Jul 2013\n-  - Jun 2013\n-  - Oct 2012\n-  - Jun 2012\n-  - Jun 2012\n\n#### 3D Computer Vision\n *  - Steve Seitz (University of Washington) 2011\n *  - Steve Seitz (University of Washington) 2013\n\n#### Internet Vision\n *  - Noah Snavely (Cornell University) 2011\n *  - Noah Snavely (Cornell University) 2014\n *  - Steve Seitz (University of Washington) 2013\n\n#### Computational Photography\n *  - Richard Szeliski (Microsoft Research) 2013\n *  - William T. Freeman (MIT) 2011\n *  -  Yair Weiss (The Hebrew University of Jerusalem) 2011\n *  -  Peyman Milanfar (UC Santa Cruz/Google) 2010\n *  Andrew Blake (Microsoft Research) 2007\n *  - William T. Freeman (MIT) 2012\n *  - Fr\xc3\xa9do Durand (MIT) 2012\n *  - Rich Radke (Rensselaer Polytechnic Institute) 2014\n\n#### Learning and Vision\n *  - William T. Freeman (MIT) 2011\n *  - Simon Lucey (CMU) 2008\n *  - Yair Weiss (The Hebrew University of Jerusalem) 2009\n\n#### Object Recognition\n *  - Larry Zitnick (Microsoft Research)\n *  - Fei-Fei Li (Stanford University)\n\n#### Graphical Models\n *  - Pedro Felzenszwalb (Brown University) 2012\n *  - Zoubin Ghahramani (University of Cambridge) 2009\n *  - Sam Roweis (NYU) 2006\n *  -  Yair Weiss (The Hebrew University of Jerusalem) 2009\n\n#### Machine Learning\n *  - Jeff A. Bilmes (UC Berkeley) 1998\n *  - Christopher Bishop (Microsoft Research) 2009\n *  - Chih-Jen Lin (National Taiwan University) 2006\n *  - Michael I. Jordan (UC Berkeley)\n\n#### Optimization\n *  - Stephen J. Wright (University of Wisconsin-Madison)\n *  - Lieven Vandenberghe (University of California, Los Angeles)\n *  - Andrew Fitzgibbon (Microsoft Research)\n *  - Francis Bach (INRIA)\n *  - Daniel Cremers (Technische Universit\xc3\xa4t M\xc3\xbcnchen) ()\n\n#### Deep Learning\n *  - Geoffrey E. Hinton (University of Toronto)\n *  -  Ruslan Salakhutdinov (University of Toronto)\n *  - Yoshua Bengio (University of Montreal)\n *  -  Alex Krizhevsky (University of Toronto)\n *  Yann LeCun (NYU/Facebook Research) 2014\n *  - Rob Fergus (NYU/Facebook Research)\n *  - St\xc3\xa9phane Mallat (Ecole Normale Superieure)\n *  - IPAM, 2012\n * \n *  - Reykjavik, Iceland 2014\n\t*  - Yoshua Bengio (Universtiy of Montreal)\n\t*  - Yoshua Bengio (University of Montreal)\n\t*  - Yoshua Bengio (University of Montreal)\n\n## Software\n\n#### Annotation tools\n* \n* \n* \n* \n\n#### External Resource Links\n *  - Jia-Bin Huang (UIUC)\n *  - CVPapers\n *  - Xin Li (West Virginia University)\n * \n\n#### General Purpose Computer Vision Library\n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n\n#### Multiple-view Computer Vision\n* \n* \n*  - geometric computer vision algorithms\n*  - Minimal problems solver\n* \n* \n* \n*  - Multiple View Geometry; Structure from Motion library & softwares\n* \n* \n* \n* \n* \n\n\n#### Feature Detection and Extraction\n* \n* \n  * David G. Lowe, ""Distinctive image features from scale-invariant keypoints,"" International Journal of Computer Vision, 60, 2 (2004), pp. 91-110.\n* \n* \n  * Stefan Leutenegger, Margarita Chli and Roland Siegwart, ""BRISK: Binary Robust Invariant Scalable Keypoints"", ICCV 2011\n* \n  * Herbert Bay, Andreas Ess, Tinne Tuytelaars, Luc Van Gool, ""SURF: Speeded Up Robust Features"", Computer Vision and Image Understanding (CVIU), Vol. 110, No. 3, pp. 346--359, 2008\n* \n  * A. Alahi, R. Ortiz, and P. Vandergheynst, ""FREAK: Fast Retina Keypoint"", CVPR 2012\n* \n  * Pablo F. Alcantarilla, Adrien Bartoli and Andrew J. Davison, ""KAZE Features"", ECCV 2012\n* \n\n#### High Dynamic Range Imaging\n* \n\n#### Semantic Segmentation\n* \n\n#### Low-level Vision\n\n###### Stereo Vision\n * \n * \n * \n * \n\n###### Optical Flow\n * \n * \n * \n * \n *  - Ce Liu (MIT)\n * \n * \n * \n\n###### Image Denoising\nBM3D, KSVD,\n\n###### Super-resolution\n * \n    * Pickup, L. C. Machine Learning in Multi-frame Image Super-resolution, PhD thesis 2008\n * \n    * W. T Freeman and C. Liu. Markov Random Fields for Super-resolution and Texture Synthesis. In A. Blake, P. Kohli, and C. Rother, eds., Advances in Markov Random Fields for Vision and Image Processing, Chapter 10. MIT Press, 2011\n * \n    * K. I. Kim and Y. Kwon, ""Single-image super-resolution using sparse regression and natural image prior"", IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 32, no. 6, pp. 1127-1133, 2010.\n * \n    * T. Peleg and M. Elad, A Statistical Prediction Model Based on Sparse Representations for Single Image Super-Resolution, IEEE Transactions on Image Processing, Vol. 23, No. 6, Pages 2569-2582, June 2014\n * \n    * R. Zeyde, M. Elad, and M. Protter On Single Image Scale-Up using Sparse-Representations, Curves & Surfaces, Avignon-France, June 24-30, 2010 (appears also in Lecture-Notes-on-Computer-Science - LNCS).\n * \n    * Jianchao Yang, John Wright, Thomas Huang, and Yi Ma. Image super-resolution via sparse representation. IEEE Transactions on Image Processing (TIP), vol. 19, issue 11, 2010.\n * \n    * H. Chang, D.Y. Yeung, Y. Xiong. Super-resolution through neighbor embedding. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), vol.1, pp.275-282, Washington, DC, USA, 27 June - 2 July 2004.\n * \n    *  Yu Zhu, Yanning Zhang and Alan Yuille, Single Image Super-resolution using Deformable Patches, CVPR 2014\n * \n    * Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, Learning a Deep Convolutional Network for Image Super-Resolution, in ECCV 2014\n * \n    * R. Timofte, V. De Smet, and L. Van Gool. A+: Adjusted Anchored Neighborhood Regression for Fast Super-Resolution, ACCV 2014\n * \n    * Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja, Single Image Super-Resolution using Transformed Self-Exemplars, IEEE Conference on Computer Vision and Pattern Recognition, 2015\n\n###### Image Deblurring\n\nNon-blind deconvolution\n * \n * \n * \n * \n * \n * \n\nBlind deconvolution\n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n\nNon-uniform Deblurring\n * \n * \n * \n * \n\n\n###### Image Completion\n * \n * \n * \n * \n\n###### Image Retargeting\n * \n\n###### Alpha Matting\n * \n * \n * \n * \n * \n\n###### Image Pyramid\n* \n* \n\n###### Edge-preserving image processing\n * \n * \n * \n * \n * \n * \n * \n * \n * \n\n#### Intrinsic Images\n\n* \n* \n\n#### Contour Detection and Image Segmentation\n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n\n#### Interactive Image Segmentation\n * \n * \n * \n * \n * \n * \n\n#### Video Segmentation\n * \n * \n * \n * \n\n#### Camera calibration\n * \n * \n * \n\n#### Simultaneous localization and mapping\n\n###### SLAM community:\n * \n * \n\n###### Tracking/Odometry:\n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n * \n\n###### Graph Optimization:\n *  -- Georgia Institute of Technology\n * \n\n###### Loop Closure:\n *  - also available in \n * \n\n###### Localization & Mapping:\n * \n * \n * \n\n#### Single-view Spatial Understanding\n *  - Derek Hoiem (CMU)\n *  - Varsha Hedau (UIUC)\n *  - David C. Lee (CMU)\n *  - Ruiqi Guo (UIUC)\n\n#### Object Detection\n * \n * \n * \n * \n * \n * \n * \n * \n * \n\n#### Nearest Neighbor Search\n\n###### General purpose nearest neighbor search\n * \n * \n * \n\n###### Nearest Neighbor Field Estimation\n * \n * \n * \n * \n * \n\n#### Visual Tracking\n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n\n#### Saliency Detection\n\n#### Attributes\n\n#### Action Reconition\n\n#### Egocentric cameras\n\n#### Human-in-the-loop systems\n\n#### Image Captioning\n *  -\n\n#### Optimization\n *  - Nonlinear least-square problem and unconstrained optimization solver\n * - Nonlinear least-square problem and unconstrained optimization solver\n *  - Factor graph based discrete optimization and inference solver\n *  - Factor graph based lease-square optimization solver\n\n#### Deep Learning\n * \n\n#### Machine Learning\n * \n * \n * \n\n## Datasets\n\n#### External Dataset Link Collection\n *  - CVPapers\n *  - Which paper provides the best results on standard dataset X?\n * \n * \n * \n * \n * \n * \n * \n\n#### Low-level Vision\n\n###### Stereo Vision\n * \n * \n * \n * \n\n###### Optical Flow\n * \n * \n * \n * \n\n###### Video Object Segmentation\n * \n * \n\n\n###### Change Detection\n * \n * \n\n###### Image Super-resolutions\n * \n\n#### Intrinsic Images\n * \n * \n * \n\n#### Material Recognition\n * \n * \n * \n\n#### Multi-view Reconsturction\n* \n\n#### Saliency Detection\n\n#### Visual Tracking\n * \n * \n * \n * \n * \n\n#### Visual Surveillance\n * \n * \n\n#### Saliency Detection\n\n#### Change detection\n * \n\n#### Visual Recognition\n\n###### Image Classification\n * \n * \n\n###### Self-supervised Learning\n* \n\n###### Scene Recognition\n * \n * \n\n###### Object Detection\n * \n * \n * \n\n###### Semantic labeling\n * \n * \n * \n * \n\n###### Multi-view Object Detection\n * \n * \n * \n * \n * \n * \n\n###### Fine-grained Visual Recognition\n * \n * \n\n###### Pedestrian Detection\n * \n * \n\n#### Action Recognition\n\n###### Image-based\n\n###### Video-based\n * \n * \n\n###### Image Deblurring\n * \n * \n\n#### Image Captioning\n * \n * \n * \n\n#### Scene Understanding\n #  - A RGB-D Scene Understanding Benchmark Suite\n #  - Indoor Segmentation and Support Inference from RGBD Images\n\n#### Aerial images\n #  - Learning Aerial Image Segmentation From Online Maps\n\n\n## Resources for students\n\n#### Resource link collection\n *  - Fr\xc3\xa9do Durand (MIT)\n *  - Aaron Hertzmann (Adobe Research)\n *  - Yashar Ganjali, Aaron Hertzmann (University of Toronto)\n *  - Simon Peyton Jones (Microsoft Research)\n *  - Tao Xie (UIUC) and Yuan Xie (UCSB)\n\n#### Writing\n *  - Fr\xc3\xa9do Durand (MIT)\n *  - Fr\xc3\xa9do Durand (MIT)\n *  - Fr\xc3\xa9do Durand (MIT)\n *  - William T. Freeman (MIT)\n *  - Simon Peyton Jones (Microsoft Research)\n *  - SIGGRAPH ASIA 2011 Course\n *  - Aaron Hertzmann (Adobe Research)\n *  - Jim Blinn\n *  - Jim Kajiya (Microsoft Research)\n *  - Li-Yi Wei (The University of Hong Kong)\n *  - Martin Martin Hering Hering--Bertram (Hochschule Bremen University of Applied Sciences)\n *  - Takeo Igarashi (The University of Tokyo)\n *  - Marc H. Raibert (Boston Dynamics, Inc.)\n *  - Derek Hoiem (UIUC)\n *  - Wojciech Jarosz (Dartmouth College)\n\n\n#### Presentation\n *  - Fr\xc3\xa9do Durand (MIT)\n *  - David Fleet (University of Toronto) and Aaron Hertzmann (Adobe Research)\n *  - Colin Purrington\n\n#### Research\n *  - William T. Freeman (MIT)\n *  - Richard Hamming\n *  - Yi Ma (UIUC)\n *  - Robert L. Park\n *  - Thomas Funkhouser (Cornell University)\n *  - David Chapman (MIT)\n *  - Ming-Hsuan Yang (UC Merced)\n *  - Jia-Bin Huang (UIUC)\n *  - Jia-Bin Huang (UIUC)\n\n#### Time Management\n *  - Randy Pausch (CMU)\n\n## Blogs\n *  - Satya Mallick\n *  - Tomasz Malisiewicz\n *  - Vincent Spruyt\n *  - Andrej Karpathy\n *  - Utkarsh Sinha\n *  - Eugene Khvedchenya\n *  - Jason Chin (University of Western Ontario)\n\n\n## Links\n*  - David Lowe\n* \n* \n* \n* \n* \n*\n## Songs\n* \n* \n* \n\n## Licenses\nLicense\n\n\n\nTo the extent possible under law,  has waived all copyright and related or neighboring rights to this work.\n'"
"SpecuCheck is a Windows utility for checking the state of the software mitigations and hardware against  CVE-2017-5754 (Meltdown), CVE-2017-5715 (Spectre v2), CVE-2018-3260 (Foreshadow), and CVE-2018-3639 (Spectre v4)","b""# SpecuCheck\n\nSpecuCheck is a Windows utility for checking the state of the software and hardware mitigations against CVE-2017-5754 (Meltdown), CVE-2017-5715 (Spectre v2), CVE-2018-3260 (Foreshadow), and CVE-2018-3639 (Spectre v4). It uses two new information classes that were added to the NtQuerySystemInformation API call as part of the recent patches introduced in January 2018 and reports the data as seen by the Windows Kernel. \n\nAn  Microsoft Powershell Cmdlet Module now exists as well, which is the recommended and supported way to get this information.\n\n## Screenshots\n\n\n\n## Introduction\n\nOn January 3rd 2018, Intel, AMD and ARM Holdings, as well as a number of OS Vendors reported a series of vulnerabilities that were discovered by Google Project Zero:\n\n* Variant 1: bounds check bypass (CVE-2017-5753)\n* Variant 2: branch target injection (CVE-2017-5715)\n* Variant 3: rogue data cache load (CVE-2017-5754)\n\nMicrosoft released patches for Windows 7 SP1 and higher later that same day. These patches, depending on architecture, OS version, boot settings and a number of hardware-related properties, apply a number of software and hardware mitigations against these issues. The enablement state of these mitigations, their availability, and configuration is stored by the Windows kernel in a number of global variables, and exposed to user-mode callers through an undocumented system call.\n\nAdditionally, new side channel attacks were reported, such as Spectre Variant 4: speculative store bypass (CVE-2018-3639) and Foreshadow: L1 terminal fault (CVE-2018-3620) which were fixed in Windows 7 SP1 and higher with patches in August's Patch Tuesday.\n\nSpecuCheck takes advantage of this system call in order to confirm if a system has indeed been patched (non-patched systems will fail the call) and what the status of the mitigations are, which can be used to determine potential performance pitfalls.\n\n## Motivation\n\nThere was originally a lot of noise, hype, and marketing around the issue, and not a lot of documentation on how to see if you were affected, and at what performance overhead. SpecuCheck aimed to make that data easily accessible by users and IT departments, to avoid having to use a Windows debugger or reverse engineer the API themselves.\n\nSince then, Microsoft has done great work to expose that data from the kernel-mode in a concise matter, which succinctly indicates the kernel's support and usage of the various mitigating technologies and hardware features, and released a PowerShell CmdLet Module to retrieve that data. SpecuCheck, therefore, remains only as a research tool and is not recommended -- please use the Microsoft-approved PowerShell Module instead.\n\n## Installation on Windows\n\nTo run SpecuCheck, simply execute it on the command-line:\n\n\n\nWhich will result in an informational screen indicating which features/mitigations are enabled. If you see the text:\n\n\n\nThis indicates that your system is not currently patched to mitigate against these vulnerabilities.\n\n## References\n\nIf you would like to know more about my research or work, I invite you to check out my blog at  as well as my training & consulting company, Winsider Seminars & Solutions Inc., at .\n\nYou should also definitely read the incredibly informative .\n\nFor additional information on the appropriate and required Windows patches, please read the  and additional  .\n\n## Caveats\n\nSpecuCheck relies on undocumented system calls and information classes which are subject to change. Additionally, SpecuCheck only returns the information that the Windows Kernel is storing about the state of the mitigations and hardware features -- based on policy settings (registry, boot parameters) or other compatibility flags, the Windows Kernel's state may not match the true hardware state. The goal of this tool is to give you a Windows-specific assessment, not a hardware assessment that is OS-agnostic.\n\nSpecuCheck is only a research tool and is not recommended for general or commercial use -- please use the Microsoft-approved PowerShell Module instead.\n\n## License\n\n\n"""
CLI for the Apple Dev Center,"b'\n\nNote: Cupertino stopped working due to a recent change on the Apple Developer Portal. A maintained alternative is to use  to communicate with Apples Developer back-end, or use any of the , like  or .\n\n------\n\nAutomate administrative tasks that you would normally have to do through the Apple Dev Center websites. Lifes too short to manage device identifiers by hand!\n\n> Cupertino is named after : home to Apple, Inc.s world headquarters.\n> Its part of a series of world-class command-line utilities for iOS development, which includes  (Building & Distribution),  (Push Notifications),  (In-App Purchase Receipt Verification),  (Passbook pass generation), and  (iTunes Store API).\n\n## Requirements\n\nCupertino requires the , which can be installed with the following command:\n\n\n\n## Installation\n\n\n\n## Usage\n\n### Authentication\n\n\n\n_Credentials are saved in the Keychain. You will not be prompted for your username or password by commands while you are logged in. (Mac only)\n\nAlternatively, username and password can also be provided by setting the\n\n\n\n\n\n\n_Adds (without an editor) a list of devices to a provisioning profile_\n\n\n\n_Removes (without an editor) a list of devices from a provisioning profile_\n\n---\n\n\n\n### App IDs\n\n\n\n\n\n### Certificates\n\n\n\n\n\n## CSV Output\n\nThe following commands will format their output as  when the  argument is passed:\n\n- \n- \n- \n- \n\n## Commands\n\n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n\n## Options\n\nGlobal options:\n\n-  : Username\n-  : Password\n-  : Team Identifier\n-  : Set log level to INFO\n-  : Set log level to DEBUG\n-  : Output options are  or  \n\nSpecific options for certain commands:\n\n-  : Options are  or \n\n## Proxies\n\nCupertino will access the provisioning portal through a proxy if the  environment variable is set, with optional credentials  and .\n\n## License\n\nCupertino is released under an MIT license. See LICENSE for more information.\n'"
Community-based GPL-licensed network monitoring system,"b'\n\nIntroduction\n------------\n\nLibreNMS is an auto-discovering PHP/MySQL/SNMP based network monitoring\nwhich includes support for a wide range of network hardware and operating\nsystems including Cisco, Linux, FreeBSD, Juniper, Brocade, Foundry, HP and\nmany more.\n\nWe intend LibreNMS to be a viable project and community that:\n- encourages contribution,\n- focuses on the needs of its users, and\n- offers a welcoming, friendly environment for everyone.\n\nThe [Debian Social Contract][10] will be the basis of our priority system,\nand mutual respect is the basis of our behavior towards others.\n\n\nDocumentation\n-------------\n\nDocumentation can be found in the [doc directory][5] or [docs.librenms.org][16], including instructions\nfor installing and contributing.\n\n\nParticipating\n-------------\n\nYou can participate in the project by:\n- Talking to us on [Discord][4] or [Twitter][3].\n- Joining the \n- Improving the [documentation][5].\n- Cloning the [repository][2] and filing [pull requests][19] on GitHub.\n-  on our Community Forums\n- See [CONTRIBUTING][15] for more details.\n\n\nVM image\n--------\n\nYou can try LibreNMS by downloading a VM image.  Currently, a Ubuntu-based\nimage is supplied and has been tested with [VirtualBox][8].\n\nDownload one of the [VirtualBox images][11] we have available, documentation is provided which details\nlogin credentials and setup details.\n\nLicense\n-------\n\nCopyright (C) 2006-2012 Adam Armstrong \n\nCopyright (C) 2013-2023 by individual LibreNMS contributors\n\n This program is free software: you can redistribute it and/or modify\n it under the terms of the GNU General Public License as published by\n the Free Software Foundation, either version 3 of the License, or\n (at your option) any later version.\n\n This program is distributed in the hope that it will be useful,\n but WITHOUT ANY WARRANTY; without even the implied warranty of\n MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n GNU General Public License for more details.\n\n You should have received a copy of the GNU General Public License\n along with this program.  If not, see .\n\n[LICENSE.txt][14] contains a copy of the full GPLv3 licensing conditions.\n\nThe following additional license conditions apply to LibreNMS (a GPL\nexception):\n\n  As a special exception, you have permission to link or otherwise combine\n  LibreNMS with the included copies of the following third-party software,\n  and distribute modified versions, as long as you follow the requirements\n  of the GNU GPL v3 in regard to all of the remaining software (comprising\n  LibreNMS).\n\n  Please see [Acknowledgements][17]\n\n[2]: https://github.com/librenms/librenms ""Main LibreNMS GitHub repo""\n[3]: https://twitter.com/librenms ""@LibreNMS on Twitter""\n[4]: https://discord.gg/librenms ""Discord LibreNMS Server""\n[5]: https://github.com/librenms/librenms/tree/master/doc/\n[8]: https://www.virtualbox.org/ ""VirtualBox""\n[10]: http://www.debian.org/social_contract ""Debian project social contract""\n[11]: https://www.librenms.org/#downloads\n[14]: https://github.com/librenms/librenms/tree/master/LICENSE.txt\n[15]: https://docs.librenms.org/General/Contributing/\n[16]: https://docs.librenms.org/\n[17]: https://docs.librenms.org/General/Acknowledgement/\n[19]: https://github.com/librenms/librenms/pulls\n\n\n## Backers\n\nSupport us with a monthly donation and help us continue our activities. []\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Sponsors\n\nBecome a sponsor and get your logo on our README on GitHub with a link to your site. []\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
A flexible framework of neural networks for deep learning,"b'[<marko.inline.RawText object at 0x000001F254324CD0>, <marko.inline.Link object at 0x000001F254327B10>, <marko.inline.RawText object at 0x000001F254324050>]\n\n----\n\n\n\n# Chainer: A deep learning framework\n\n\n\n\n\n\n\n\n\n| \n| \n| Tutorials ()\n| Examples (, )\n| \n| \n\nForum (, )\n| Slack invitation (, )\n| Twitter (, )\n\nChainer is a Python-based deep learning framework aiming at flexibility.\nIt provides automatic differentiation APIs based on the define-by-run approach (a.k.a. dynamic computational graphs) as well as object-oriented high-level APIs to build and train neural networks.\nIt also supports CUDA/cuDNN using  for high performance training and inference.\nFor more details about Chainer, see the documents and resources listed above and join the community in Forum, Slack, and Twitter.\n\n## Installation\n\nFor more details, see the \n\nTo install Chainer, use .\n\n\n\nTo enable CUDA support,  is required.\nRefer to the .\n\n\n## Docker image\n\nWe are providing the official Docker image.\nThis image supports .\nLogin to the environment with the following command, and run the Python interpreter to use Chainer with CUDA and cuDNN support.\n\n\n\n\n## Contribution\n\nSee the .\n\n\n## ChainerX\n\nSee the .\n\n\n## License\n\nMIT License (see  file).\n\n\n## More information\n\n- \n\n## References\n\nTokui, Seiya, et al. ""Chainer: A Deep Learning Framework for Accelerating the Research Cycle."" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2019.\n \n\nTokui, S., Oono, K., Hido, S. and Clayton, J.,\nChainer: a Next-Generation Open Source Framework for Deep Learning,\nProceedings of Workshop on Machine Learning Systems(LearningSys) in\nThe Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS), (2015)\n, \n\nAkiba, T., Fukuda, K. and Suzuki, S.,\nChainerMN: Scalable Distributed Deep Learning Framework,\nProceedings of Workshop on ML Systems in\nThe Thirty-first Annual Conference on Neural Information Processing Systems (NIPS), (2017)\n, \n'"
Easily craft fast Neural Networks on iOS! Use TensorFlow models. Metal under the hood.,"b'# Bender\n\n\n\n\n\n\n\n\n\n\n\n\n\nBender is an abstraction layer over MetalPerformanceShaders useful for working with neural networks.\n\n## Contents\n* \n* \n* \n* \n* \n* \n* \n* \n\nThe documentation can be found under the  folder:\n*  contains the most important information to get started.\n* [Supported Layers] explains which layers are supported and how they map to TensorFlow ops.\n* [Importing] explains how to import models from other frameworks such as TensorFlow. You can also find information on how to enhance this functionality for custom implementations.\n\n## Introduction\n\nBender is an abstraction layer over MetalPerformanceShaders which is used to work with neural networks. It is of growing interest in the AI environment to execute neural networks on mobile devices even if the training process has been done previously. We want to make it easier for everyone to execute pretrained networks on iOS.\n\nBender allows you to easily define and run neural networks using the most common layers like Convolution, Pooling, FullyConnected and some normalizations among others. It is also flexible in the way it receives the parameters for these layers.\n\nWe also want to support loading models trained on other frameworks such as TensorFlow or Caffe2. Currently Bender includes an adapter for TensorFlow that loads a graph with variables and ""translates"" it to Bender layers. This feature supports a subset of TensorFlows operations but we plan to enhance it to cover more cases.\n\n## Why did we need Bender? \n\nAt [Xmartlabs] we were about to start a Machine Learning project and investigated frameworks to use in iOS. We found MetalPerformanceShaders useful but not very user friendly and we saw ourselves repeating a lot of code and information. That is why we starting building a framework to handle that kind of stuff.\n\nWe also found ourselves creating scripts to translate the models we had from training with TensorFlow to iOS. This means transposing the weights to the MPSCNN format and also mapping the parameters of the different kinds of layers in TensorFlow to the parameters used by the MPSCNN kernels. TensorFlow can be compiled for iOS but currently it does not support running on GPU which we wanted to do. We also did not want to include TensorFlows static library into our project. This is why we also started to work on an adapter that would parse a TF graph and translate it to our Bender layers.\n\n## Usage\n\nYou can define your own network in Bender using our custom operator or you can load a model exported from TensorFlow. Defining a network and loading a model can be done like this:\n\n\n\nYou can read more information about this in .\n\nIf you want to define your network yourself you can do it like this:\n\n\n\nand once youre done with all your layers:\n\n\n\nTo know more about this have a look at .\n\n\n## Requirements\n\n* Xcode 9\n* iOS 11.0+ (but deployment target is iOS 10.0, so iOS 10 is supported)\n\n## Getting involved\n\n* If you want to contribute please feel free to submit pull requests.\n* If you have a feature request please open an issue.\n* If you found a bug or need help please check older issues, .\n\nBefore contribute check the [CONTRIBUTING] file for more info.\n\nIf you use Bender in your app We would love to hear about it! Drop us a line on .\n\n## Examples\n\nFollow these steps to run the examples:\n* Clone Bender repository (or download it).\n* Run  in the downloaded folder.\n* Open Bender workspace and run the Example project.\n\n> There is an Image recognition example which includes a MobileNet model in Bender and one in CoreML. It is also set up to run an Inception model but you will have to download it separately as it is almost 100 MB in size.\nYou can download it from http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz but then you have to freeze it and add it to the Example Xcode project as inception_v3.pb.\n\n## Installation\n\n#### CocoaPods\n\nTo install Bender, simply add the following line to your Podfile:\n\n\n\n> Remember that Bender compiles for iOS 10. So you must add  to your Podfile\n\n#### Carthage\n\n is a simple, decentralized dependency manager for Cocoa.\n\nTo install Bender, add the following line to your Cartfile:\n\n\n\nThen run:\n\n\n\nFinally, drag the built  binaries for ,  and  to your applications Xcode project.\n\n## Author\n\n*  ()\n\n\n# Change Log\n\nThis can be found in the  file.\n\n\n[Xmartlabs]: http://xmartlabs.com\n[Importing]: Documentation/Importing.md\n[CONTRIBUTING]: .github/CONTRIBUTING.md\n[API]: Documentation/API.md\n[Supported Layers]: Documentation/Supported_Layers.md\n\n\n## License\n\n\n## Citation\nIf you use this code in your research please cite us:\n\n\n'"
markdown preview plugin for (neo)vim,"b' \xe2\x9c\xa8 Markdown Preview for (Neo)vim \xe2\x9c\xa8 \n\n> Powered by \xe2\x9d\xa4\xef\xb8\x8f\n\n### Introduction\n\n> It only works on Vim >= 8.1 and Neovim\n\nPreview Markdown in your modern browser with synchronised scrolling and flexible configuration.\n\nMain features:\n\n- Cross platform (MacOS/Linux/Windows)\n- Synchronised scrolling\n- Fast asynchronous updates\n-  for typesetting of math\n- \n- \n- \n- \n- \n- \n- \n- Emojis\n- Task lists\n- Local images\n- Flexible configuration\n\nNote the plugin  is not needed for typesetting math.\n\n\n\n### Installation & Usage\n\nInstall with :\n\n\n\nOr install with :\n\n\n\nOr with :\n\n\n\nOr with :\n\nPlace this in your  or ,\n\n... then run the following in Vim (to complete the  installation):\n\nOr with :\n\nAdd this in your \n\n\nOr with :\n\nAdd this in your \n\n\n\nOr by hand:\n\n\n\nadd plugin to the  directory:\n\n\n\nPlease make sure that you have installed  and .\nOpen  and run  to make it workable\n\n### MarkdownPreview Config:\n\n\n\nMappings:\n\n\n\nCommands:\n\n\n\n### Custom Examples\n\nTable of contents\n\n> one of\n\n    ${toc}\n    [[toc]]\n    [toc]\n    [[toc]]\n\nImage Size:\n\n\n\nPlantUML:\n\n    @startuml\n    Bob -> Alice : hello\n    @enduml\n\nOr\n\n    \n\nKaTeX:\n\n    $sqrt{3x-1}+(1+x)^2$\n\n    $$begin{array}{c}\n\n    nabla times vec{mathbf{B}} -, frac1c, frac{partialvec{mathbf{E}}}{partial t} &\n    = frac{4pi}{c}vec{mathbf{j}}    nabla cdot vec{mathbf{E}} & = 4 pi rho \n\n    nabla times vec{mathbf{E}}, +, frac1c, frac{partialvec{mathbf{B}}}{partial t} & = vec{mathbf{0}} \n\n    nabla cdot vec{mathbf{B}} & = 0\n\n    end{array}$$\n\nmermaid:\n\n    \n\njs-sequence-diagrams:\n\n    \nFlowchart:\n\n    \n\ndot:\n\n    \n\nchart:\n\n    \n\n### FAQ\n\n#### Why is the synchronised scrolling lagging?\n\nSet  to a small number, for instance: \n\nWSL 2 issue: Can not open browser when using WSL 2 with terminal Vim.\n\n> if you are using Ubuntu you can install xdg-utils using \n> checkout  for more detail.\n\n#### How can I change the dark/light theme?\n\nThe default theme is based on your system preferences.\nThere is a button hidden in the header to change the theme. Place your mouse over the header to reveal it.\n\n#### How can I pass CLI options to the browser, like opening in a new window?\n\nAnswer: Add the following to your Neovim init script:\n\nLinux\n\nReplace  with  if you prefer. Both browsers recognize the  option.\n\nmacOS\n\nReplace  with  or  if you prefer. They all recognize the  option.\n\n### About Vim Support\n\nVim support is powered by \n\n### References\n\n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n\n### Buy Me A Coffee \xe2\x98\x95\xef\xb8\x8f\n\n\n\n\n'"
"A chat server with OAuth2 authentication, persistent and searchable history, video and audio, markdown formatting, private and public rooms, stars, votes, embedded games, and many other features","b""\n\n\nA chat server with authentication, persistent and searchable history, rich markdown formatting, video, private rooms, conversation highlighting, plugins, persisted notifications, code and table rendering, specialized link boxing, github hooks, bots, and many other features.\n\n\n\n\n\n\n****\n\nYou can see it in action or use it on https://miaou.dystroy.org (anybody can create a room for public or private use on this server).\n\n# Installing a server\n\nIf you want to install Miaou, the installation documentation is available .\n\nDon't hesitate to come ask us some advices in one of the dedicated chat rooms  and  (please note that they're currently more active between 8 and 19 GMT).\n\nAnd if you run your own server, please tell us, we're always happy to learn about installations.\n\n# Contributing\n\nAs described in , Miaou is mostly coded in JavaScript.\n\nStuff includes node, PostgreSQL, OAuth2, socket.io, WebRTC, express, Bluebird, Redis, Pug, Passport.js, hu.js, jQuery, sass/scss, Uglify-js, gulp, jest, travis-ci, and nginx.\n\nMany features are implemented as plugins, and that's where you should look first: .\n\nIf you have the ability and will to contribute, come and discuss with us. The best landing place is usually the  where you can ping @dystroy or @Florian.\n\nHelp is welcome but remember:\n\n1. Come and discuss with us before to code\n2. And, always test yourself and run the test suite before doing a pull request.\n\n## License\n\nMost of Miaou follows the . Exceptions are specified .\n\nCopyright (c) 2014 Denys S\xc3\xa9guret <>\n"""
Polycode is a cross-platform framework for creative code.,"b'\n\nPolycode is a cross-platform framework for creative code. You can use it as a C++ API or as a standalone scripting language to get easy and simple access to accelerated 2D and 3D graphics, hardware shaders, sound and network programming, physics engines and more.\n\nThe core Polycode API is written in C++ and can be used to create portable native applications.\n\nOn top of the core C++ API, Polycode offers a Lua-based scripting system with its own set of compilation tools. The Lua API mirrors the C++ API and can be used to easily create prototypes and even publish complete applications to multiple platforms without compiling C++.\n\nPolycode is available under the MIT license and was designed and developed by Ivan Safrin.\n\nPlease see BUILD.md for instructions on how to build Polycode from source.\n'"
An express-based Node.js web application bootstrapping module.,"b'\n\n# kraken.js\n\n   \n\nKraken builds upon  and enables environment-aware, dynamic configuration, advanced middleware capabilities, security, and app lifecycle events.\nFor more information and examples check out \n\nTable of Contents\n=================\n\n* \n* \n  * \n* \n* \n  * \n    * \n  * \n    * \n    * \n  * \n  * \n  * \n  * \n* \n* \n* \n\n## Basic Usage\n\n\n\n\n## API\n\n\n\nkraken-js is used just like any normal middleware, however it does more than just return a function; it configures a\ncomplete express 4 application. See below for a list of features, but to get started just use it like middleware.\n\n\n\n### Options\nPass the following options to kraken via a config object such as this:\n\n\nNote: All kraken-js configuration settings are optional.\n\n####  (String, optional)\nThe working directory for kraken to use. kraken loads configuration files,\nroutes, and registers middleware so this directory is the path against all relative paths are resolved. The default value\nis the directory of the file that uses kraken, which is generally  (or ).\n\n####  (Function, optional)\nProvides an asynchronous hook for loading additional configuration. When invoked, a\n configuration object containing all loaded configuration value passed\nas the first argument, and a callback as the second. The signature of this handler is \nand the callback is a standard error-back which accepts an error as the first argument and the config object as the\nsecond, e.g. .\n\n####  (Object, optional)\nProtocol handler implementations for use when processing configuration. For more information on protocols\nsee  and .\nBy default, kraken comes with a set of shortstop protocols which are described in the ""Config Protocols"" section below,\nbut you can add your own by providing an object with the protocol names as the keys and their implementations as\nproperties, for example:\n\n\n####  (Function, optional)\nProvides a synchronous hook which executes once kraken mounts. It takes an express  instance as the first argument, and  as the second. The signature of this handler is .\n\n####  (Function, optional)\nHandler for  errors outside of the middleware chain. See the  module for defaults.\n\nFor uncaught errors in the middleware chain, see  middleware instead.\n\n####  (Object, optional)\nIn rare cases, it may be useful to pass options directly to the confit module used within .\nFor example, if , you can explicitly ignore those environment variables:\n\n\n## Config Protocols\nkraken comes with the following shortstop protocol handlers by default:\n#### \nMerge the contents of the specified file into configuration under a given key.\n\n\n#### \nReplace with the value at a given key. Note that the keys in this case are dot (.) delimited.\n\n\n#### \nThe path handler is documented in the  repo.\n\n#### \nThe file handler is documented in the  repo.\n\n#### \nThe base64 handler is documented in the  repo.\n\n#### \nThe env handler is documented in the  repo.\n\n#### \nThe require handler is documented in the  repo.\n\n#### \nThe exec handler is documented in the  repo.\n\n#### \nThe glob handler is documented in the  repo.\n\n#### \nThe resolve handler is documented in the  repo.\n\n\n\n\n## Features\n\n\n### Configuration\n\n\n#### Environment-aware\n\nUsing environment suffixes, configuration files are applied and overridden according to the current environment as set\nby . The application looks for a  directory relative to the basedir and looks for  as the baseline config specification. JSON files matching the current env are processed and loaded. Additionally, JSON configuration files may contain comments.\n\nValid  values are  or  (uses ),  (uses ),  (uses ),  (uses ). Simply add a config file with the name, to have it read only in that environment, e.g. .\n\n\n### Middleware\n\nMuch like configuration, you shouldnt need to write a lot of code to determine whats in your middleware chain.  is used internally to read,\nresolve, and register middleware with your express application. You can either specify the middleware in your  or , (or) import it from a separate json file using the import protocol mentioned above.\n\n#### Included Middleware\nKraken comes with common middleware already included in its  file. The following is a list of the included middleware and their default configurations which can be overridden in your apps configuration:\n*  - internal middleware which handles graceful shutdowns in production environments\n  - Priority - 0\n  - Enabled -  if not in a development environment\n  - Module - \n    - Arguments (Array)\n      - Object\n        -  - milliseconds (default: )\n        -  - template to render (default: )\n        -  - custom headers to write while still disconnecting.\n        -  - custom handler -  - for uncaught errors. Default behavior is to log the error and then trigger shutdown.\n*  - adds compression to server responses\n  - Priority - 10\n  - Enabled -  (disabled in all environments by default)\n  - Module -  ()\n*  - serves the sites favicon\n  - Priority - 30\n  - Module -  ()\n    - Arguments (Array)\n      - String - local path to the favicon file (default: )\n*  - serves static files from a specific folder\n  - Priority - 40\n  - Module -  ()\n    - Arguments (Array)\n      - String - local path to serve static files from (default: )\n*  - logs requests and responses\n  - Priority - 50\n  - Module -  ()\n    - Arguments (Array)\n      - String - log format type (default: )\n*  - parses JSON request bodies\n  - Priority - 60\n  - Module -  ()\n    - Method - \n*  - parses URL Encoded request bodies\n  - Priority - 70\n  - Module -  ()\n    - Method - \n    - Arguments (Array)\n      - Object\n        -  (Boolean) - parse extended syntax with the  module (default: )\n*  - parses multipart FORM bodies\n  - Priority - 80\n  - Module -  (delegates to )\n*  - parses cookies in request headers\n  - Priority - 90\n  - Module -  ()\n    - Arguments (Array)\n      - String - secret used to sign cookies (default: )\n*  - maintains session state\n  - Priority - 100\n  - Module -  ()\n    - Arguments (Array)\n      - Object\n        -  (String) - cookie name (default: )\n        -  (String) - secret used to sign session cookie (default: )\n        -  (Object) - describing options for the session cookie\n          -  (String) - base path to verify cookie (default: )\n          -  (Boolean) - value indicating inaccessibility of cookie in the browser (default: )\n          -  (Number) - expiration of the session cookie (default: )\n        -  (Boolean) - value indicating whether sessions should be saved even if unmodified (default: )\n        -  (Boolean) - value indicating whether to save uninitialized sessions (default: )\n        -  (Boolean) - value indicating whether to trust the reverse proxy (default: , inherit from )\n*  - secures the application against common vulnerabilities (see Application Security below)\n  - Priority - 110\n  - Module -  ()\n    - Arguments (Array)\n      - Object\n        -  (Boolean|Object) - value indicating whether to require CSRF tokens for non GET, HEAD, or OPTIONS requests, or an options object to configure CSRF protection (default: )\n        -  (String) - value for the  header (default: )\n        -  (String|Boolean) - the Compact Privacy Policy value or  if not used (default: )\n        -  (Object|Boolean) - options configuring Content Security Policy headers or  if not used (default: )\n*  - routes traffic to the applicable controller\n  - Priority - 120\n  - Module -  ()\n    - Arguments (Array)\n      - Object\n        -  (String) - path to the single file to load (default: )\n\nAdditional notes:\n- The session middleware defaults to using the in-memory store. This is not recommended for production applications and the configuration should be updated to use a shared resource (such as Redis or Memcached) for session storage.\n- You can change the routes which are affected by the middleware by providing a top-level option of . In express deployments, it is common to re-route where static files are served which can be accomplished like so:\n\n\n\n#### Extending Default Middleware\nIn any non-trivial Kraken deployment you will likely need to extend the included middleware. Common middleware which need extension include cookie parsing and session handling. In those particular cases, the secrets used should be updated:\n\n\n\nAnother common update is to pass options to middleware which is configured only with the defaults, such as the compression middleware:\n\n\n\nMore complicated examples include configuring the session middleware to use a shared resource, such as . This requires a few extra steps, most notably creating your own middleware to handle the registration (see  for a complete example):\n\n  1. Overlay the existing session middleware in your configuration:\n\n  \n\n  2. Add your custom middleware for Kraken to configure:\n\n  \n\n\n### Application Security\n\nKraken uses  to secure your applications, so that you dont need to think about it. Techniques like CSRF, XFRAMES, and CSP are enabled automatically while others can be opted into. All are customizable through configuration.\n\n\n### Lifecycle Events\n\nKraken adds support for additional events to your express app instance:\n\n*  - the application has safely started and is ready to accept requests\n*  - the application is shutting down, no longer accepting requests\n*  - the http server is no longer connected or the shutdown timeout has expired\n\n\n\n### Configuration-based  Settings\nSince express instances are themselves config objects, the convention is to set values on the app instance for use by\nexpress internally as well as other code across the application. kraken-js allows you to configure express via JSON.\nAny properties are supported, but kraken-js defaults include:\n\n\nAdditional notes:\n- The  setting will be set to the environment value as derived by kraken-js, so what is put here will be overwritten\nat runtime.\n- Set the  property to the one of the  property names (see the section )\nto enable it for template rendering.\n- The optional  property is a special case in which you can set a path to a module which exports a constructor implementing\nthe view API as defined by the module . If set, kraken-js will attempt to load the specified module and\nconfigure express to use it for resolving views.\n\nFor example:\n\n\n\n\n### View Engine Configuration\nkraken-js looks to the  config property to understand how to load and initialize renderers. The value of the\n property is an object mapping the desired file extension to engine config settings. For example:\n\n\nThe available engine configuration options are:\n\n-  (String) - This is the node module that provides the renderer implementation. The value can be the name of a\nmodule installed via npm, or it can be a module in your project referred to via file path, for example .\n-  (String, optional) - Set this if the name of the rendering engine is different from the desired file extension.\nFor example, you chose to use ejs, but want to use the ""html"" file extension for your templates. Additionally, if the\nrenderer function exported by the module is not the file extension and a ""renderer"" property is not defined, this value will be used.\n-  (String|Object, optional) - The renderer property allows you to explicitly identify the property or the\nfactory method exported by the module that should be used when settings the renderer. Set the value to a String to identify\nthat the renderer is exported by that name, or an object with the properties ""method"" and ""arguments"" to identify a factory method.\nFor example, using ejs you could set this property to ""renderFile"" or ""__express"" as the ejs module exports a renderer directly.\n\n## Tests\n\n\n## Coverage\n\n\n## Reading app configs from within the kraken app\n\nThere are two different ways. You can\n\n* Read it in your  handler as mentioned above.\n\n* Read it off the  object by doing . So it would look like:\n\n'"
E2E test framework for Angular apps,"b'Protractor   \n==========\n\n is an end-to-end test framework for  and  applications. Protractor is a  program built on top of . Protractor runs tests against your application running in a real browser, interacting with it as a user would.\n\nCompatibility\n-------------\n\nProtractor 5 is compatible with nodejs v6 and newer.\n\nProtractor works with AngularJS versions greater than 1.0.6/1.1.4, and is compatible with Angular applications. Note that for Angular apps, the  and  locators are not supported. We recommend using .\n\n\nGetting Started\n---------------\nSee the  for most documentation.\n\nTo get set up and running quickly:\n - Work through the \n - See the \n\nOnce you are familiar with the tutorial, you\xe2\x80\x99re ready to move on. To modify your environment, see the Protractor Setup docs. To start writing tests, see the Protractor Tests docs.\n\nTo better understand how Protractor works with the Selenium WebDriver and Selenium Server see the reference materials.\n\n\nGetting Help\n------------\n\nCheck the  and read through the .\n\nPlease ask usage and debugging questions on  (use the  tag), the  chat room, or in the . (Please do not ask support questions here on Github.)\n\n\nFor Contributors\n----------------\nSee \n'"
Simple JSON Object mapping written in Swift,"b'ObjectMapper\n============\n\n\n\n\n\nObjectMapper is a framework written in Swift that makes it easy for you to convert your model objects (classes and structs) to and from JSON. \n\n- \n- \n- \n- \n- \n- \n- \n-  \n- \n- \n- \n- \n- \n\n# Features:\n- Mapping JSON to objects\n- Mapping objects to JSON\n- Nested Objects (stand alone, in arrays or in dictionaries)\n- Custom transformations during mapping\n- Struct support\n- \n\n# The Basics\nTo support mapping, a class or struct just needs to implement the  protocol which includes the following functions:\n\nObjectMapper uses the  operator to define how each member variable maps to and from JSON.\n\n\n\nOnce your class implements , ObjectMapper allows you to easily convert to and from JSON. \n\nConvert a JSON string to a model object:\n\n\nConvert a model object to a JSON string:\n\n\nAlternatively, the  class can also be used to accomplish the above (it also provides extra functionality for other situations):\n\n\nObjectMapper can map classes composed of the following types:\n- \n- \n- \n- \n- \n-  (Enums)\n- \n- \n- \n- \n- \n-  \n- \n- \n- Optionals of all the above\n- Implicitly Unwrapped Optionals of the above\n\n##  Protocol\n\n####  \nThis function is where all mapping definitions should go. When parsing JSON, this function is executed after successful object creation. When generating JSON, it is the only function that is called on the object.\n\n####  \nThis failable initializer is used by ObjectMapper for object creation. It can be used by developers to validate JSON prior to object serialization. Returning nil within the function will prevent the mapping from occuring. You can inspect the JSON stored within the  object to do your validation:\n\n\n##  Protocol\n is an alternative to . It provides developers with a static function that is used by ObjectMapper for object initialization instead of . \n\nNote: , like , is a sub protocol of  which is where the  function is defined.\n\n####  \nObjectMapper uses this function to get objects to use for mapping. Developers should return an instance of an object that conforms to  in this function. This function can also be used to:\n- validate JSON prior to object serialization\n- provide an existing cached object to be used for mapping\n- return an object of another type (which also conforms to ) to be used for mapping. For instance, you may inspect the JSON to infer the type of object that should be used for mapping ()\n\nIf you need to implement ObjectMapper in an extension, you will need to adopt this protocol instead of . \n\n##  Protocol\n\n provides the ability to map immutable properties. This is how  differs from :\n\n\n  \n    ImmutableMappable\n    Mappable\n  \n  \n    Properties\n  \n  \n    \n\nlet id: Int\nlet name: String?\n\n  \n    \n\nvar id: Int!\nvar name: String?\n\n    \n  \n  \n    JSON -> Model\n  \n  \n    \n\ninit(map: Map) throws {\n  id   = try map.value(""id"")\n  name = try? map.value(""name"")\n}\n\n  \n    \n\nmutating func mapping(map: Map) {\n  id   <- map[""id""]\n  name <- map[""name""]\n}\n\n    \n  \n  \n    Model -> JSON\n  \n  \n    \n\nfunc mapping(map: Map) {\n  id   >>> map[""id""]\n  name >>> map[""name""]\n}\n\n    \n    \n\nmutating func mapping(map: Map) {\n  id   <- map[""id""]\n  name <- map[""name""]\n}\n\n    \n  \n  \n    Initializing\n  \n  \n    \n\ntry User(JSONString: JSONString)\n\n    \n    \n\nUser(JSONString: JSONString)\n\n    \n  \n\n\n#### \n\nThis throwable initializer is used to map immutable properties from the given . Every immutable property should be initialized in this initializer.\n\nThis initializer throws an error when:\n-  fails to get a value for the given key\n-  fails to transform a value using \n\n uses  method to get values from the . This method should be used with the  keyword as it is throwable.  properties can easily be handled using .\n\n\n\n#### \n\nThis method is where the reverse transform is performed (model to JSON). Since immutable properties cannot be mapped with the  operator, developers have to define the reverse transform using the  operator.\n\n\n\n# Easy Mapping of Nested Objects\nObjectMapper supports dot notation within keys for easy mapping of nested objects. Given the following JSON String:\n\nYou can access the nested objects as follows:\n\nNested keys also support accessing values from an array. Given a JSON response with an array of distances, the value could be accessed as follows:\n\nIf you have a key that contains , you can individually disable the above feature as follows:\n\nWhen you have nested keys which contain , you can pass the custom nested key delimiter as follows ():\n\n\n# Custom Transforms\nObjectMapper also supports custom transforms that convert values during the mapping process. To use a transform, simply create a tuple with  and the transform of your choice on the right side of the  operator:\n\nThe above transform will convert the JSON Int value to an Date when reading JSON and will convert the Date to an Int when converting objects to JSON.\n\nYou can easily create your own custom transforms by adopting and implementing the methods in the  protocol:\n\n\n### TransformOf\nIn a lot of situations you can use the built-in transform class  to quickly perform a desired transformation.  is initialized with two types and two closures. The types define what the transform is converting to and from and the closures perform the actual transformation. \n\nFor example, if you want to transform a JSON  value to an  you could use  as follows:\n\nHere is a more condensed version of the above:\n\n\n# Subclasses\n\nClasses that implement the  protocol can easily be subclassed. When subclassing mappable classes, follow the structure below:\n\n\n\nMake sure your subclass implementation calls the right initializers and mapping functions to also apply the mappings from your superclass.\n\n# Generic Objects\n\nObjectMapper can handle classes with generic types as long as the generic type also conforms to . See the following example:\n\n\n# Mapping Context\n\nThe  object which is passed around during mapping, has an optional  object that is available for developers to use if they need to pass information around during mapping. \n\nTo take advantage of this feature, simply create an object that implements  (which is an empty protocol) and pass it into  during initialization. \n\n\n# ObjectMapper + Alamofire\n\nIf you are using  for networking and you want to convert your responses to Swift objects, you can use . It is a simple Alamofire extension that uses ObjectMapper to automatically map JSON response data to Swift objects.\n\n\n# ObjectMapper + Realm\n\nObjectMapper and Realm can be used together. Simply follow the class structure below and you will be able to use ObjectMapper to generate your Realm models:\n\n\n\nIf you want to serialize associated RealmObjects, you can use . It is a simple Realm extension that serializes arbitrary JSON into Realms  class.\n\nTo serialize Swift , ,  and  arrays you can use . Itll wrap Swift types into RealmValues that can be stored in Realms  class.\n\nNote: Generating a JSON string of a Realm Object using ObjectMappers  function only works within a Realm write transaction. This is because ObjectMapper uses the  flag in its mapping functions () which are used both for serializing and deserializing. Realm detects the flag and forces the  function to be called within a write block even though the objects are not being modified.\n\n# Projects Using ObjectMapper\n- \n\n- \n\n-   \n\nIf you have a project that utilizes, extends or provides tooling for ObjectMapper, please submit a PR with a link to your project in this section of the README.\n\n# To Do\n- Improve error handling. Perhaps using \n- Class cluster documentation\n\n# Contributing\n\nContributions are very welcome \xf0\x9f\x91\x8d\xf0\x9f\x98\x83. \n\nBefore submitting any pull request, please ensure you have run the included tests and they have passed. If you are including new functionality, please write test cases for it as well.\n\n# Installation\n### Cocoapods\nObjectMapper can be added to your project using  by adding the following line to your :\n\n\n\n### Carthage\nIf youre using  you can add a dependency on ObjectMapper by adding it to your :\n\n\n\n### Swift Package Manager\nTo add ObjectMapper to a  based project, add:\n\n\nto your  files  array.\n\n### Submodule\nOtherwise, ObjectMapper can be added as a submodule:\n\n1. Add ObjectMapper as a  by opening the terminal, -ing into your top-level project directory, and entering the command \n2. Open the  folder, and drag  into the file navigator of your app project.\n3. In Xcode, navigate to the target configuration window by clicking on the blue project icon, and selecting the application target under the ""Targets"" heading in the sidebar.\n4. Ensure that the deployment target of  matches that of the application target.\n5. In the tab bar at the top of that window, open the ""Build Phases"" panel.\n6. Expand the ""Target Dependencies"" group, and add .\n7. Click on the  button at the top left of the panel and select ""New Copy Files Phase"". Rename this new phase to ""Copy Frameworks"", set the ""Destination"" to ""Frameworks"", and add .\n'"
A Python Interpreter written in Rust,"b'\n\n# \n\nA Python-3 (CPython >= 3.12.0) Interpreter written in Rust :snake: :scream:\n:metal:.\n\n\n\n\n\n[][discord]\n\n\n\n\n\n\n## Usage\n\nCheck out our \n\nRustPython requires Rust latest stable version (e.g 1.67.1 at February 7th 2023). If you dont\ncurrently have Rust installed on your system you can do so by following the instructions at .\n\nTo check the version of Rust youre currently running, use . If you wish to update,\n will update your Rust installation to the most recent stable release.\n\nTo build RustPython locally, first, clone the source code:\n\n\n\nThen you can change into the RustPython directory and run the demo (Note:  is\nneeded to prevent stack overflow on Windows):\n\n\n\nOr use the interactive shell:\n\n\n\nNOTE: For windows users, please set  environment variable as  path in project directory.\n(e.g. When RustPython directory is , set  as )\n\nYou can also install and run RustPython with the following:\n\n\n\nIf youd like to make https requests, you can enable the  feature, which\nalso lets you install the  package manager. Note that on Windows, you may\nneed to install OpenSSL, or you can enable the  feature instead,\nwhich compiles OpenSSL for you but requires a C compiler, perl, and .\n\nOnce youve installed rustpython with SSL support, you can install pip by\nrunning:\n\n\n\nYou can also install RustPython through the  package manager, though\nthis isnt officially supported and may be out of date:\n\n\n\n### WASI\n\nYou can compile RustPython to a standalone WebAssembly WASI module so it can run anywhere.\n\nBuild\n\n\n\nRun by wasmer\n\n\n\nRun by wapm\n\n\n\n#### Building the WASI file\n\nYou can build the WebAssembly WASI file with:\n\n\n\n> Note: we use the  to include the standard library inside the binary. You also have to run once .\n\n### JIT (Just in time) compiler\n\nRustPython has a very experimental JIT compiler that compile python functions into native code.\n\n#### Building\n\nBy default the JIT compiler isnt enabled, its enabled with the  cargo feature.\n\n\n\nThis requires autoconf, automake, libtool, and clang to be installed.\n\n#### Using\n\nTo compile a function, call  on it.\n\n\n\n## Embedding RustPython into your Rust Applications\n\nInterested in exposing Python scripting in an application written in Rust,\nperhaps to allow quickly tweaking logic where Rusts compile times would be inhibitive?\nThen  and  may be of some assistance.\n\n## Disclaimer\n\nRustPython is in development, and while the interpreter certainly can be used\nin interesting use cases like running Python in WASM and embedding into a Rust\nproject, do note that RustPython is not totally production-ready.\n\nContribution is more than welcome! See our contribution section for more\ninformation on this.\n\n## Conference videos\n\nCheckout those talks on conferences:\n\n- \n- \n\n## Use cases\n\nAlthough RustPython is a fairly young project, a few people have used it to\nmake cool projects:\n\n- : an open-source, cloud-native, distributed time-series database. Using RustPython for embedded scripting.\n- : a game engine written in\n  rust.\n- : an arena-based AI competition platform\n- : an extremely fast Python linter, written in Rust\n\n## Goals\n\n- Full Python-3 environment entirely in Rust (not CPython bindings)\n- A clean implementation without compatibility hacks\n\n## Documentation\n\nCurrently along with other areas of the project, documentation is still in an\nearly phase.\n\nYou can read the  for the\nlatest release, or the .\n\nYou can also generate documentation locally by running:\n\n\n\nDocumentation HTML files can then be found in the  directory or you can append  to the previous commands to\nhave the documentation open automatically on your default browser.\n\nFor a high level overview of the components, see the  document.\n\n## Contributing\n\nContributions are more than welcome, and in many cases we are happy to guide\ncontributors through PRs or on Discord. Please refer to the\n as well for tips on developments.\n\nWith that in mind, please note this project is maintained by volunteers, some of\nthe best ways to get started are below:\n\nMost tasks are listed in the\n. Check issues\nlabeled with  if you wish to start coding.\n\nTo enhance CPython compatibility, try to increase unittest coverage by checking this article: \n\nAnother approach is to checkout the source code: builtin functions and object\nmethods are often the simplest and easiest way to contribute.\n\nYou can also simply run  to assist in finding any unimplemented\nmethod.\n\n## Compiling to WebAssembly\n\n\n\n## Community\n\n[][discord]\n\nChat with us on [Discord][discord].\n\n## Code of conduct\n\nOur code of conduct .\n\n## Credit\n\nThe initial work was based on\n and\n\n\n[discord]: https://discord.gg/vru8NypEhv\n\n## Links\n\nThese are some useful links to related projects:\n\n- https://github.com/ProgVal/pythonvm-rust\n- https://github.com/shinglyu/RustPython\n- https://github.com/windelbouwman/rspython\n\n## License\n\nThis project is licensed under the MIT license. Please see the\n file for more details.\n\nThe  is licensed under the CC-BY-4.0\nlicense. Please see the  file\nfor more details.\n'"